{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8acd22-9c08-48c1-80d5-4b424f610f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем необходимые библиотеки\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a93f28-9e2f-4d96-a689-aa5f2c2851c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установим случайные начальные значения для воспроизводимости\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c1cd6-8556-4b7f-9bb1-f0b113ae42a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим доступность GPU видеокарты\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используемое устройство: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5d6fe-4ac2-4754-b908-b5d4e64aa92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Предварительная обработка данных\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    \"\"\"Датасет для работы с видео и их описаниями\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dir, caption_file, vocab, max_frames=40):\n",
    "        \"\"\"Инициализация датасета\n",
    "        \n",
    "        Аргументы:\n",
    "            feature_dir (str): Путь к директории с предвычисленными признаками\n",
    "            caption_file (str): Файл с описаниями в формате \"video_id описание\"\n",
    "            vocab (Vocabulary): Объект словаря для токенизации\n",
    "            max_frames (int): Макс. количество кадров на видео\n",
    "        \"\"\"\n",
    "        self.feature_dir = feature_dir\n",
    "        self.max_frames = max_frames\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # Загрузка и парсинг описаний\n",
    "        self.captions = self._load_captions(caption_file)\n",
    "        self.video_ids = list(self.captions.keys())\n",
    "    \n",
    "    def _load_captions(self, caption_file):\n",
    "        \"\"\"Загружает описания из файла\"\"\"\n",
    "        captions = {}\n",
    "        with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(' ', 1)\n",
    "                if len(parts) == 2:\n",
    "                    video_id, caption = parts\n",
    "                    captions.setdefault(video_id, []).append(caption)\n",
    "        return captions\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Получает один элемент датасета по индексу\"\"\"\n",
    "        video_id = self.video_ids[idx]\n",
    "        \n",
    "        # Загрузка предвычисленных признаков\n",
    "        features = self._load_features(video_id)\n",
    "        \n",
    "        # Выбор случайного описания и токенизация\n",
    "        caption = self._process_caption(video_id)\n",
    "        \n",
    "        return features, caption\n",
    "    \n",
    "    def _load_features(self, video_id):\n",
    "        \"\"\"Загружает признаки видео из файла\"\"\"\n",
    "        feature_path = os.path.join(self.feature_dir, f\"{video_id}.npy\")\n",
    "        features = np.load(feature_path)\n",
    "        \n",
    "        # Проверяем и корректируем размерность\n",
    "        if features.ndim > 2:            \n",
    "            features = features.reshape(features.shape[0], -1) # Преобразуем к [seq_len, feature_dim]\n",
    "        return torch.FloatTensor(features).to(device)\n",
    "    \n",
    "    def _process_caption(self, video_id):\n",
    "        \"\"\"Токенизирует и преобразует описание в тензор\"\"\"\n",
    "        caption = np.random.choice(self.captions[video_id])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        caption = [self.vocab('<start>')] + [self.vocab(token) for token in tokens] + [self.vocab('<end>')]\n",
    "        return torch.LongTensor(caption)\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Словарь для преобразования слов в индексы\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "        self._add_special_tokens()\n",
    "    \n",
    "    def _add_special_tokens(self):\n",
    "        \"\"\"Добавляет специальные токены\"\"\"\n",
    "        for token in ['<pad>', '<start>', '<end>', '<unk>']:\n",
    "            self.add_word(token)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        \"\"\"Добавляет слово в словарь\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __call__(self, word):\n",
    "        \"\"\"Возвращает индекс слова или токен <unk>\"\"\"\n",
    "        return self.word2idx.get(word, self.word2idx['<unk>'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(caption_file, threshold=4):\n",
    "    \"\"\"Строит словарь на основе файла с описаниями\"\"\"\n",
    "    counter = Counter()\n",
    "    \n",
    "    # Подсчет частот слов\n",
    "    with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(' ', 1)\n",
    "            if len(parts) == 2:\n",
    "                counter.update(nltk.tokenize.word_tokenize(parts[1].lower()))\n",
    "    \n",
    "    # Фильтрация по порогу\n",
    "    vocab = Vocabulary()\n",
    "    for word, count in counter.items():\n",
    "        if count >= threshold:\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def precompute_features(video_dir, output_dir, batch_size=16):\n",
    "    \"\"\"Предварительно вычисляет признаки видео с помощью ResNet152\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Инициализация модели для извлечения признаков\n",
    "    model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "    feature_extractor = nn.Sequential(*list(model.children())[:-1]).to(device)\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    # Трансформации для кадров\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Обработка видеофайлов\n",
    "    video_files = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi'))]\n",
    "    \n",
    "    for i in range(0, len(video_files), batch_size):\n",
    "        batch_files = video_files[i:i + batch_size]\n",
    "        \n",
    "        for video_file in tqdm(batch_files, desc=f\"Batch {i//batch_size + 1}\"):\n",
    "            \n",
    "            video_id = os.path.splitext(video_file)[0]\n",
    "            feature_path = os.path.join(output_dir, f\"{video_id}.npy\") # Проверяем, существуют ли уже признаки\n",
    "            if os.path.exists(feature_path):\n",
    "                continue  # Признаки уже есть, пропускаем\n",
    "\n",
    "            video_path = os.path.join(video_dir, video_file)\n",
    "            # Извлечение кадров\n",
    "            frames = extract_frames(video_path)\n",
    "            \n",
    "            # Извлечение признаков\n",
    "            features = []\n",
    "            with torch.no_grad():\n",
    "                for frame in frames:\n",
    "                    if frame.ndim == 3:  # Проверка валидности кадра\n",
    "                        frame = transform(frame).unsqueeze(0).to(device)\n",
    "                        feature = feature_extractor(frame).squeeze().cpu().numpy()\n",
    "                        features.append(feature)\n",
    "            \n",
    "            # Сохранение признаков\n",
    "            features = np.stack(features, axis=0)\n",
    "            np.save(os.path.join(output_dir, f\"{video_id}.npy\"), features)\n",
    "\n",
    "def extract_frames(video_path, max_frames=40):\n",
    "    \"\"\"Извлекает кадры из видео с равномерной дискретизацией\"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video: {video_path}\")\n",
    "    \n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    sample_rate = max(1, frame_count // max_frames)\n",
    "    frames = []\n",
    "    \n",
    "    for i in range(0, frame_count, sample_rate):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        if len(frames) >= max_frames:\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Дополнение нулями при необходимости\n",
    "    if len(frames) < max_frames:\n",
    "        frames.extend([np.zeros_like(frames[0]) for _ in range(max_frames - len(frames))])\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acbf7b6-63ec-473f-ae48-6421261cff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Извлечение признаков\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Класс для извлечения признаков из видеокадров с использованием предобученной CNN\n",
    "    Основные функции:\n",
    "    - Инициализация предобученной модели CNN (по умолчанию ResNet152)\n",
    "    - Преобразование входных кадров к нужному формату\n",
    "    - Извлечение признаков из каждого кадра\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn_model=None):\n",
    "        if cnn_model is None:\n",
    "            # Загрузка предобученной ResNet152\n",
    "            cnn_model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "            # Удаляем последний классификационный слой\n",
    "            self.model = nn.Sequential(*list(cnn_model.children())[:-1])\n",
    "        else:\n",
    "            self.model = cnn_model\n",
    "\n",
    "        # Перенос модели на устройство (GPU/CPU) и перевод в режим оценки\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Определение преобразований для входных изображений\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),               # Конвертация в PIL Image\n",
    "            transforms.Resize((224, 224)),         # Изменение размера под вход сети\n",
    "            transforms.ToTensor(),                 # Конвертация в тензор\n",
    "            transforms.Normalize(                  # Нормализация\n",
    "                mean=[0.485, 0.456, 0.406],        # Средние значения ImageNet\n",
    "                std=[0.229, 0.224, 0.225]          # Стандартные отклонения ImageNet\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def extract_features(self, frames):\n",
    "        \"\"\"\n",
    "        Извлекает признаки из списка кадров\n",
    "        \n",
    "        Аргументы:\n",
    "            frames (list): Список кадров в формате numpy arrays\n",
    "            \n",
    "        Возвращает:\n",
    "            torch.Tensor: Извлеченные признаки размерности [число_кадров, размерность_признака]\n",
    "            \n",
    "        Процесс работы:\n",
    "        1. Применение преобразований к каждому кадру\n",
    "        2. Извлечение признаков с помощью CNN\n",
    "        3. Накопление и объединение признаков\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        with torch.no_grad(): # Отключаем вычисление градиентов для ускорения\n",
    "            for frame in frames:\n",
    "                # Применяем преобразования и добавляем batch-размерность\n",
    "                frame = self.transform(frame).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Извлекаем признаки\n",
    "                feature = self.model(frame)\n",
    "                feature = feature.squeeze() # Удаляем лишние размерности\n",
    "                \n",
    "                features.append(feature.cpu()) # Переносим на CPU для экономии памяти\n",
    "        \n",
    "        return torch.stack(features) # Объединяем все признаки в один тензор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c85705-37f0-4994-8cbc-9bc6f04010a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Архитектура модели\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Видео-энкодер для обработки признаков кадров с учетом временной информации\n",
    "    Использует двунаправленный LSTM для анализа последовательности кадров\n",
    "    \n",
    "    Основные функции:\n",
    "    - Обработка признаков отдельных кадров\n",
    "    - Учет временных зависимостей между кадрами\n",
    "    - Подготовка скрытых состояний для декодера\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, hidden_dim, num_layers=1, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Инициализация энкодера\n",
    "        \n",
    "        Аргументы:\n",
    "            feature_dim (int): Размерность входных признаков кадра\n",
    "            hidden_dim (int): Размерность скрытого слоя LSTM\n",
    "            num_layers (int): Количество слоев LSTM\n",
    "            dropout (float): Вероятность дропаута\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Двунаправленный LSTM для временного кодирования\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feature_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,          # Первая размерность - batch\n",
    "            bidirectional=True,       # Двунаправленная архитектура\n",
    "            dropout=dropout if num_layers > 1 else 0  # Дропаут только для многослойных LSTM\n",
    "        )\n",
    "        \n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Прямой проход через энкодер\n",
    "        \n",
    "        Аргументы:\n",
    "            features (torch.Tensor): Признаки видеокадров размерности \n",
    "                                    [batch_size, seq_len, feature_dim]\n",
    "            \n",
    "        Возвращает:\n",
    "            tuple: (outputs, hidden)\n",
    "                - outputs: Выходы LSTM [batch_size, seq_len, hidden_dim*2]\n",
    "                - hidden: Кортеж (скрытое состояние, состояние ячейки)\n",
    "        \"\"\"\n",
    "         # Проверяем размерность входных данных\n",
    "        if features.dim() > 3:\n",
    "            batch_size, seq_len = features.size(0), features.size(1)\n",
    "            # Преобразуем к [batch_size, seq_len, feature_dim]\n",
    "            features = features.view(batch_size, seq_len, -1)\n",
    "            \n",
    "        # Убеждаемся, что последнее измерение имеет правильный размер\n",
    "        if features.size(-1) != self.feature_dim:\n",
    "            raise ValueError(f\"Неверная размерность признаков: ожидается {self.feature_dim}, получено {features.size(-1)}\")\n",
    "        \n",
    "        # Пропускаем признаки через LSTM\n",
    "        outputs, hidden = self.lstm(features)\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Слой внимания для выделения наиболее релевантных частей видео\n",
    "    Реализует механизм внимания на основе скалярного произведения\n",
    "    \n",
    "    Основные функции:\n",
    "    - Вычисление весов внимания для каждого кадра\n",
    "    - Создание контекстного вектора\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_dim, decoder_dim):\n",
    "        \"\"\"\n",
    "        Инициализация слоя внимания\n",
    "        \n",
    "        Аргументы:\n",
    "            encoder_dim (int): Размерность выхода энкодера\n",
    "            decoder_dim (int): Размерность скрытого состояния декодера\n",
    "        \"\"\"\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        # Линейные преобразования для вычисления внимания        \n",
    "        self.encoder_attn = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.full_attn = nn.Linear(decoder_dim, 1)\n",
    "        \n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Прямой проход через слой внимания\n",
    "        \n",
    "        Аргументы:\n",
    "            encoder_outputs (torch.Tensor): Выходы энкодера [batch_size, seq_len, encoder_dim]\n",
    "            decoder_hidden (torch.Tensor): Скрытое состояние декодера [batch_size, decoder_dim]\n",
    "            \n",
    "        Возвращает:\n",
    "            tuple: (context, attention_weights)\n",
    "                - context: Контекстный вектор [batch_size, encoder_dim]\n",
    "                - attention_weights: Веса внимания [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        # Проецируем выходы энкодера в пространство декодера\n",
    "        # Размерность: [batch_size, seq_len, decoder_dim]\n",
    "        attn_proj = self.encoder_attn(encoder_outputs)\n",
    "        \n",
    "        # Добавляем размерность для совместимости\n",
    "        # Размерность: [batch_size, 1, decoder_dim]\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1)\n",
    "        \n",
    "        # Вычисляем оценки внимания через тангенс\n",
    "        # Размерность: [batch_size, seq_len, 1]\n",
    "        attn_scores = self.full_attn(torch.tanh(attn_proj + decoder_hidden))\n",
    "        \n",
    "        # Нормализуем оценки в веса с помощью softmax\n",
    "        # Размерность: [batch_size, seq_len]\n",
    "        attn_weights = F.softmax(attn_scores.squeeze(2), dim=1)\n",
    "        \n",
    "        # Вычисляем взвешенную сумму выходов энкодера\n",
    "        # Размерность: [batch_size, encoder_dim]\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        \n",
    "        return context, attn_weights\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Декодер подписей, генерирующий описание слово за словом\n",
    "    Использует механизм внимания для фокусировки на релевантных частях видео\n",
    "    \n",
    "    Основные компоненты:\n",
    "    - Слой эмбеддинга слов\n",
    "    - Механизм внимания\n",
    "    - LSTM ячейка\n",
    "    - Выходной слой\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, encoder_dim, hidden_dim, attention_dim, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Инициализация декодера\n",
    "        \n",
    "        Аргументы:\n",
    "            vocab_size (int): Размер словаря\n",
    "            embed_dim (int): Размерность эмбеддинга слов\n",
    "            encoder_dim (int): Размерность выхода энкодера\n",
    "            hidden_dim (int): Размерность скрытого слоя LSTM\n",
    "            attention_dim (int): Размерность механизма внимания\n",
    "            dropout (float): Вероятность дропаута\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Слой эмбеддинга слов\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Слой внимания\n",
    "        self.attention = AttentionLayer(encoder_dim, hidden_dim)\n",
    "        \n",
    "        # LSTM ячейка\n",
    "        self.lstm = nn.LSTMCell(embed_dim + encoder_dim, hidden_dim)\n",
    "        \n",
    "        # Слой дропаута\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Выходной полносвязный слой\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Инициализация весов\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Инициализация весов для эмбеддингов и выходного слоя.\"\"\"\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)  # Равномерная инициализация\n",
    "        self.fc.bias.data.fill_(0)                      # Нулевые смещения\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)         # Равномерная инициализация\n",
    "        \n",
    "    def init_hidden_state(self, batch_size):\n",
    "        \"\"\"\n",
    "        Инициализация скрытого состояния LSTM\n",
    "        \n",
    "        Аргументы:\n",
    "            batch_size (int): Размер батча\n",
    "            \n",
    "        Возвращает:\n",
    "            tuple: (скрытое состояние, состояние ячейки)\n",
    "        \"\"\"\n",
    "        h = torch.zeros(batch_size, self.hidden_dim).to(device)\n",
    "        c = torch.zeros(batch_size, self.hidden_dim).to(device)\n",
    "        return h, c\n",
    "        \n",
    "    def forward(self, encoder_outputs, captions, lengths):\n",
    "        \"\"\"\n",
    "        Прямой проход через декодер (обучение)\n",
    "        \n",
    "        Аргументы:\n",
    "            encoder_outputs (torch.Tensor): Выходы энкодера [batch_size, seq_len, encoder_dim]\n",
    "            captions (torch.Tensor): Истинные подписи [batch_size, max_caption_length]\n",
    "            lengths (list): Фактические длины подписей\n",
    "            \n",
    "        Возвращает:\n",
    "            torch.Tensor: Предсказания [batch_size, max_caption_length, vocab_size]\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        \n",
    "        # Сортируем данные по убыванию длины (для эффективности)\n",
    "        lengths, sort_idx = torch.sort(lengths, descending=True)\n",
    "        encoder_outputs = encoder_outputs[sort_idx]\n",
    "        captions = captions[sort_idx]\n",
    "        \n",
    "        # Инициализируем состояние LSTM\n",
    "        h, c = self.init_hidden_state(batch_size)\n",
    "        \n",
    "        # Определяем максимальную длину в батче\n",
    "        max_length = lengths[0].item()\n",
    "        \n",
    "        # Инициализируем тензор предсказаний\n",
    "        predictions = torch.zeros(batch_size, max_length, self.vocab_size).to(device)\n",
    "        \n",
    "        # Получаем эмбеддинги слов\n",
    "        embeddings = self.embedding(captions)\n",
    "        \n",
    "        # Инициализируем контекстный вектор\n",
    "        context, _ = self.attention(encoder_outputs, h)\n",
    "        \n",
    "        # Генерируем слова последовательно\n",
    "        for t in range(max_length):\n",
    "            # Объединяем эмбеддинг и контекстный вектор\n",
    "            lstm_input = torch.cat([embeddings[:, t], context], dim=1)\n",
    "            \n",
    "            # Прямой проход через LSTM\n",
    "            h, c = self.lstm(lstm_input, (h, c))\n",
    "            \n",
    "            # Вычисляем новый контекст\n",
    "            context, _ = self.attention(encoder_outputs, h)\n",
    "            \n",
    "            # Генерируем предсказание следующего слова\n",
    "            output = self.fc(self.dropout(h))\n",
    "            predictions[:, t] = output\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def sample(self, encoder_outputs, max_length=20):\n",
    "        \"\"\"\n",
    "        Генерация подписей для заданных выходов энкодера (инференс)\n",
    "        \n",
    "        Аргументы:\n",
    "            encoder_outputs (torch.Tensor): Выходы энкодера [batch_size, seq_len, encoder_dim]\n",
    "            max_length (int): Максимальная длина подписи\n",
    "            \n",
    "        Возвращает:\n",
    "            list: Список сгенерированных подписей (индексы слов)\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        \n",
    "        # Инициализируем состояние LSTM\n",
    "        h, c = self.init_hidden_state(batch_size)\n",
    "        \n",
    "        # Начинаем с токена <start>\n",
    "        start_idx = torch.LongTensor([self.vocab.word2idx['<start>']]).to(device)\n",
    "        embedding = self.embedding(start_idx)\n",
    "        \n",
    "        # Инициализируем контекстный вектор\n",
    "        context, _ = self.attention(encoder_outputs, h)\n",
    "        \n",
    "        # Храним сгенерированные индексы\n",
    "        sampled_ids = []\n",
    "        \n",
    "        # Генерируем слова последовательно\n",
    "        for i in range(max_length):\n",
    "            # Объединяем эмбеддинг и контекст\n",
    "            lstm_input = torch.cat([embedding, context], dim=1)\n",
    "            \n",
    "            # Прямой проход через LSTM\n",
    "            h, c = self.lstm(lstm_input, (h, c))\n",
    "            \n",
    "            # Вычисляем новый контекст\n",
    "            context, alpha = self.attention(encoder_outputs, h)\n",
    "            \n",
    "            # Генерируем следующее слово\n",
    "            output = self.fc(h)\n",
    "            predicted = output.argmax(1)\n",
    "            \n",
    "            sampled_ids.append(predicted)\n",
    "            \n",
    "            # Прерываем если достигнут токен <end>\n",
    "            if predicted.item() == self.vocab.word2idx['<end>']:\n",
    "                break\n",
    "            \n",
    "            # Обновляем вход для следующего шага\n",
    "            embedding = self.embedding(predicted)\n",
    "        \n",
    "        return torch.stack(sampled_ids, 1)\n",
    "\n",
    "class VideoCaptioningModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Полная модель генерации подписей к видео, объединяющая:\n",
    "    - Энкодер видео\n",
    "    - Декодер с механизмом внимания\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, feature_dim=2048, embed_dim=512, encoder_dim=512, \n",
    "                 decoder_dim=512, attention_dim=512, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Инициализация модели\n",
    "        \n",
    "        Аргументы:\n",
    "            vocab_size (int): Размер словаря\n",
    "            feature_dim (int): Размерность входных признаков\n",
    "            embed_dim (int): Размерность эмбеддингов слов\n",
    "            encoder_dim (int): Размерность скрытого слоя энкодера\n",
    "            decoder_dim (int): Размерность скрытого слоя декодера\n",
    "            attention_dim (int): Размерность механизма внимания\n",
    "            dropout (float): Вероятность дропаута\n",
    "        \"\"\"\n",
    "        super(VideoCaptioningModel, self).__init__()\n",
    "        \n",
    "        # Инициализация энкодера\n",
    "        self.encoder = Encoder(\n",
    "            feature_dim=feature_dim,\n",
    "            hidden_dim=encoder_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Инициализация декодера\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            encoder_dim=encoder_dim * 2,  # Учитываем двунаправленность энкодера\n",
    "            hidden_dim=decoder_dim,\n",
    "            attention_dim=attention_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"\n",
    "        Прямой проход через модель\n",
    "        \n",
    "        Аргументы:\n",
    "            features (torch.Tensor): Признаки видео [batch_size, seq_len, feature_dim]\n",
    "            captions (torch.Tensor): Истинные подписи [batch_size, max_caption_length]\n",
    "            lengths (list): Фактические длины подписей\n",
    "            \n",
    "        Возвращает:\n",
    "            torch.Tensor: Предсказания модели [batch_size, max_caption_length, vocab_size]\n",
    "        \"\"\"\n",
    "        # Кодируем видео\n",
    "        encoder_outputs, _ = self.encoder(features)\n",
    "        \n",
    "        # Декодируем подписи\n",
    "        outputs = self.decoder(encoder_outputs, captions, lengths)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48159f0b-2c46-4819-ad79-83c52920021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Функции обучения модели\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, epoch, device):\n",
    "    \"\"\"Обучение модели в течение одной эпохи\n",
    "    \n",
    "    Аргументы:\n",
    "        model: Модель для обучения\n",
    "        dataloader: Загрузчик обучающих данных\n",
    "        criterion: Функция потерь\n",
    "        optimizer: Оптимизатор\n",
    "        epoch: Номер текущей эпохи\n",
    "        device: Устройство для вычислений (CPU/GPU)\n",
    "        \n",
    "    Возвращает:\n",
    "        Среднее значение потерь за эпоху\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=f'Эпоха {epoch} [Обучение]')\n",
    "    \n",
    "    for features, captions in progress_bar:\n",
    "        # Подготовка данных\n",
    "        features = features.to(device, non_blocking=True)\n",
    "        captions = captions.to(device, non_blocking=True)\n",
    "        lengths = torch.tensor([len(cap) for cap in captions], \n",
    "                             dtype=torch.long, device='cpu')\n",
    "        \n",
    "        # Обнуление градиентов\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Прямой проход\n",
    "        outputs = model(features, captions, lengths)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), \n",
    "                        captions.view(-1))\n",
    "        \n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "        \n",
    "        # Оптимизация\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Логирование\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Валидация модели на отдельном наборе данных\n",
    "    \n",
    "    Аргументы:\n",
    "        model: Модель для валидации\n",
    "        dataloader: Загрузчик валидационных данных\n",
    "        criterion: Функция потерь\n",
    "        device: Устройство для вычислений\n",
    "        \n",
    "    Возвращает:\n",
    "        Среднее значение потерь на валидации\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, captions, lengths in tqdm(dataloader, desc='Валидация'):\n",
    "            # Подготовка данных\n",
    "            features = features.to(device, non_blocking=True)\n",
    "            captions = captions.to(device, non_blocking=True)\n",
    "\n",
    "            # Прямой проход\n",
    "            outputs = model(features, captions, lengths)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)),\n",
    "                           captions.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def train_with_mixed_precision(model, dataloader, criterion, optimizer, \n",
    "                             epoch, device, scaler, accumulation_steps=4):\n",
    "    \"\"\"Обучение с использованием смешанной точности\n",
    "    \n",
    "    Аргументы:\n",
    "        model: Модель для обучения\n",
    "        dataloader: Загрузчик данных\n",
    "        criterion: Функция потерь\n",
    "        optimizer: Оптимизатор\n",
    "        epoch: Номер эпохи\n",
    "        device: Устройство для вычислений\n",
    "        scaler: GradScaler для смешанной точности\n",
    "        accumulation_steps: Шаги накопления градиентов\n",
    "        \n",
    "    Возвращает:\n",
    "        Среднее значение потерь за эпоху\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for i, (features, captions, lengths) in enumerate(tqdm(dataloader, \n",
    "                                                desc=f'Эпоха {epoch} [FP16]')):\n",
    "        # Подготовка данных\n",
    "        features = features.to(device, non_blocking=True)\n",
    "        captions = captions.to(device, non_blocking=True)\n",
    "        \n",
    "        # Прямой проход в смешанной точности\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            outputs = model(features, captions, lengths)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)),\n",
    "                           captions.view(-1)) / accumulation_steps\n",
    "        \n",
    "        # Обратное распространение\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Обновление весов с накоплением\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def get_lr_scheduler(optimizer, warmup_epochs=5, total_epochs=100):\n",
    "    \"\"\"Создает планировщик learning rate с прогревом и косинусным затуханием\n",
    "    \n",
    "    Аргументы:\n",
    "        optimizer: Оптимизатор\n",
    "        warmup_epochs: Количество эпох прогрева\n",
    "        total_epochs: Общее количество эпох\n",
    "        \n",
    "    Возвращает:\n",
    "        Learning rate scheduler\n",
    "    \"\"\"\n",
    "    def lr_lambda(epoch):\n",
    "        # Линейный прогрев\n",
    "        if epoch < warmup_epochs:\n",
    "            return float(epoch + 1) / float(warmup_epochs)\n",
    "        # Косинусное затухание\n",
    "        progress = float(epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "    \n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e1002a-5a2c-4d52-88b3-6868c0643656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Основной цикл обучения модели\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, \n",
    "               save_dir='checkpoints', save_every=5):\n",
    "    \"\"\"Основная функция для обучения модели\n",
    "    \n",
    "    Аргументы:\n",
    "        model: Модель для обучения\n",
    "        train_loader: DataLoader для обучающих данных\n",
    "        val_loader: DataLoader для валидационных данных  \n",
    "        criterion: Функция потерь\n",
    "        optimizer: Оптимизатор\n",
    "        num_epochs: Количество эпох обучения\n",
    "        device: Устройство для вычислений (CPU/GPU)\n",
    "        save_dir: Директория для сохранения чекпоинтов\n",
    "        save_every: Частота сохранения чекпоинтов (в эпохах)\n",
    "        \n",
    "    Возвращает:\n",
    "        Обученную модель и историю обучения\n",
    "    \"\"\"\n",
    "    \n",
    "    # Создаем директорию для сохранения чекпоинтов\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Инициализируем историю обучения\n",
    "    history = {\n",
    "        'train_loss': [],  # Потери на обучении\n",
    "        'val_loss': []     # Потери на валидации\n",
    "    }\n",
    "    \n",
    "    # Инициализируем инструменты для оптимизации обучения\n",
    "    scaler = torch.amp.GradScaler('cuda')  # Для смешанной точности (FP16/FP32)\n",
    "    scheduler = get_lr_scheduler(optimizer, warmup_epochs=5, total_epochs=num_epochs)  # Планировщик learning rate\n",
    "    \n",
    "    # Цикл обучения по эпохам\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Фаза обучения (со смешанной точностью)\n",
    "        train_loss = train_with_mixed_precision(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            device=device,\n",
    "            scaler=scaler\n",
    "        )\n",
    "        \n",
    "        # Фаза валидации\n",
    "        val_loss = validate(\n",
    "            model=model,\n",
    "            dataloader=val_loader,\n",
    "            criterion=criterion,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Обновляем learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Сохраняем метрики\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # Выводим статистику\n",
    "        print(f\"Эпоха {epoch}/{num_epochs}, \"\n",
    "              f\"Ошибка обучения: {train_loss:.4f}, \"\n",
    "              f\"Ошибка валидации: {val_loss:.4f}, \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Сохраняем чекпоинт\n",
    "        if epoch % save_every == 0:\n",
    "            checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch{epoch}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),  # Состояние модели\n",
    "                'optimizer_state_dict': optimizer.state_dict(),  # Состояние оптимизатора\n",
    "                'train_loss': train_loss,  # Потери на обучении\n",
    "                'val_loss': val_loss,      # Потери на валидации\n",
    "                'lr': optimizer.param_groups[0]['lr']  # Текущий learning rate\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Чекпоинт сохранен в {checkpoint_path}\")\n",
    "    \n",
    "    # Сохраняем финальную модель\n",
    "    final_path = os.path.join(save_dir, \"final_model.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),  # Финальные веса модели\n",
    "        'history': history  # Полная история обучения\n",
    "    }, final_path)\n",
    "    print(f\"Финальная модель сохранена в {final_path}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b1b3d-9b1a-4b10-b30f-e36fe61f62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Функции оценки качества модели\n",
    "\n",
    "def predict_caption(model, video_features, vocab, max_length=20):\n",
    "    \"\"\"Генерация подписи к видео с помощью обученной модели\n",
    "    \n",
    "    Аргументы:\n",
    "        model: Обученная модель генерации подписей\n",
    "        video_features: Тензор признаков видео [1, seq_len, feature_dim]\n",
    "        vocab: Словарь для преобразования слов\n",
    "        max_length: Максимальная длина генерируемой подписи\n",
    "        \n",
    "    Возвращает:\n",
    "        tuple: (подпись, веса внимания)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Кодирование видео\n",
    "        encoder_outputs, _ = model.encoder(video_features.to(device))\n",
    "        \n",
    "        # Инициализация декодера\n",
    "        h, c = model.decoder.init_hidden_state(1)\n",
    "        input_word = torch.tensor([vocab.word2idx['<start>']]).to(device)\n",
    "        \n",
    "        predicted_words = []\n",
    "        attention_weights = []\n",
    "        \n",
    "        # Последовательная генерация\n",
    "        for _ in range(max_length):\n",
    "            # Шаг генерации\n",
    "            embedding = model.decoder.embedding(input_word)\n",
    "            context, alpha = model.decoder.attention(encoder_outputs, h)\n",
    "            lstm_input = torch.cat([embedding, context], dim=1)\n",
    "            h, c = model.decoder.lstm(lstm_input, (h, c))\n",
    "            output = model.decoder.fc(h)\n",
    "            \n",
    "            # Получение следующего слова\n",
    "            predicted_word_idx = output.argmax(1)\n",
    "            attention_weights.append(alpha.cpu().numpy())\n",
    "            \n",
    "            # Проверка на конец последовательности\n",
    "            if predicted_word_idx.item() == vocab.word2idx['<end>']:\n",
    "                break\n",
    "                \n",
    "            predicted_words.append(vocab.idx2word[predicted_word_idx.item()])\n",
    "            input_word = predicted_word_idx\n",
    "    \n",
    "    return ' '.join(predicted_words), attention_weights\n",
    "\n",
    "\n",
    "def visualize_attention(video_frames, caption, attention_weights, grid_shape=(5, 8)):\n",
    "    \"\"\"Визуализация механизма внимания модели\n",
    "    \n",
    "    Аргументы:\n",
    "        video_frames: Список кадров видео\n",
    "        caption: Сгенерированная подпись\n",
    "        attention_weights: Веса внимания\n",
    "        grid_shape: Формат сетки для отображения\n",
    "    \"\"\"\n",
    "    words = caption.split()\n",
    "    if len(words) != len(attention_weights):\n",
    "        print(\"Предупреждение: Несоответствие количества слов и весов внимания\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(len(words), 1, figsize=(12, 3*len(words)))\n",
    "    \n",
    "    for i, (word, weights) in enumerate(zip(words, attention_weights)):\n",
    "        ax = axes[i] if len(words) > 1 else axes\n",
    "        try:\n",
    "            # Реорганизация весов внимания в сетку\n",
    "            attention_grid = weights.reshape(grid_shape)\n",
    "            im = ax.imshow(attention_grid, cmap='viridis')\n",
    "            plt.colorbar(im, ax=ax)\n",
    "            ax.set_title(f\"Слово: '{word}'\")\n",
    "            ax.axis('off')\n",
    "        except ValueError as e:\n",
    "            print(f\"Ошибка визуализации: {e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_metrics(model, dataloader, vocab, device):\n",
    "    \"\"\"Комплексная оценка качества модели\n",
    "    \n",
    "    Аргументы:\n",
    "        model: Обученная модель\n",
    "        dataloader: Загрузчик данных для оценки\n",
    "        vocab: Словарь\n",
    "        device: Устройство вычислений\n",
    "        \n",
    "    Возвращает:\n",
    "        dict: Словарь с метриками качества\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, captions in tqdm(dataloader, desc='Оценка'):\n",
    "            # Генерация подписи\n",
    "            pred_caption, _ = predict_caption(model, features.to(device), vocab)\n",
    "            hypotheses.append(pred_caption.split())\n",
    "            \n",
    "            # Обработка эталонной подписи\n",
    "            ref = []\n",
    "            for idx in captions[0].cpu().numpy():\n",
    "                if idx == vocab.word2idx['<end>']:\n",
    "                    break\n",
    "                if idx not in [vocab.word2idx['<pad>'], vocab.word2idx['<start>']]:\n",
    "                    ref.append(vocab.idx2word[idx])\n",
    "            references.append([ref])\n",
    "    \n",
    "    # Вычисление метрик\n",
    "    bleu4 = nltk.translate.bleu_score.corpus_bleu(\n",
    "        references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    return {'bleu4': bleu4}\n",
    "\n",
    "\n",
    "def find_optimal_batch_size(model, sample_data, criterion, max_batch_size=16):\n",
    "    \"\"\"Автоматический подбор оптимального размера батча\n",
    "    \n",
    "    Аргументы:\n",
    "        model: Модель для тестирования\n",
    "        sample_data: Пример данных (features, captions)\n",
    "        criterion: Функция потерь для оценки результата\n",
    "        max_batch_size: Максимальный размер батча для проверки\n",
    "        \n",
    "    Возвращает:\n",
    "        int: Оптимальный размер батча\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimal_size = 1\n",
    "    features, captions, lengths = sample_data\n",
    "    \n",
    "    for bs in range(1, max_batch_size + 1, 2):\n",
    "        try:\n",
    "            # Тестовый прогон\n",
    "            batch_features = features.repeat(bs, 1, 1).to(device)\n",
    "            batch_captions = captions.repeat(bs, 1).to(device)\n",
    "            batch_lengths = lengths.repeat(bs) # Создаем batch_lengths\n",
    "\n",
    "            # Передаем все три параметра в модель\n",
    "            outputs = model(batch_features, batch_captions, batch_lengths)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), batch_captions.view(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            optimal_size = bs\n",
    "            print(f\"Батч {bs} успешно обработан\")\n",
    "            \n",
    "            # Очистка памяти\n",
    "            del batch_features, batch_captions, batch_lengths, outputs, loss\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"памяти\" in str(e).lower():\n",
    "                print(f\"Достигнут лимит памяти при батче {bs}\")\n",
    "                break\n",
    "                \n",
    "    print(f\"Оптимальный размер батча: {optimal_size}\")\n",
    "    return optimal_size\n",
    "\n",
    "def caption_collate_fn(batch):\n",
    "    \"\"\"Пользовательская функция для обработки батчей с подписями разной длины\"\"\"\n",
    "    # Сортируем батч по длине подписей (по убыванию)\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    features, captions = zip(*batch)\n",
    "    \n",
    "    # Объединяем признаки\n",
    "    features = torch.stack(features, 0)\n",
    "    \n",
    "    # Получаем длины подписей\n",
    "    lengths = torch.LongTensor([len(cap) for cap in captions])\n",
    "    \n",
    "    # Определяем максимальную длину для паддинга\n",
    "    max_length = max(lengths)\n",
    "    \n",
    "    # Создаем тензор с заполненными подписями\n",
    "    padded_captions = torch.zeros(len(captions), max_length, dtype=torch.long)\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        padded_captions[i, :end] = cap[:end]\n",
    "    \n",
    "    return features, padded_captions, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8121c-31bd-4b57-a1f4-3f9283368417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Визуализация и анализ результатов\n",
    "\n",
    "def visualize_predictions(model, dataloader, vocab, num_samples=5):\n",
    "    \"\"\"\n",
    "    Визуализация предсказаний модели для нескольких примеров\n",
    "    \n",
    "    Аргументы:\n",
    "        model: Обученная модель\n",
    "        dataloader: Загрузчик данных\n",
    "        vocab: Словарь\n",
    "        num_samples: Количество примеров для визуализации\n",
    "    \n",
    "    Процесс:\n",
    "    1. Выбирает случайные примеры из датасета\n",
    "    2. Генерирует подписи\n",
    "    3. Сравнивает с эталонными подписями\n",
    "    4. Визуализирует результаты\n",
    "    \"\"\"\n",
    "    model.eval()  # Режим оценки\n",
    "    \n",
    "    samples = []\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():  # Без вычисления градиентов\n",
    "        for features, captions, lengths in dataloader:\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "            \n",
    "            # Переносим данные на устройство\n",
    "            features = features.to(device)\n",
    "            \n",
    "            # Генерируем подпись\n",
    "            predicted_caption, attention_weights = predict_caption(model, features, vocab)\n",
    "            \n",
    "            # Преобразуем эталонную подпись\n",
    "            ground_truth = []\n",
    "            for idx in captions[0].cpu().numpy():\n",
    "                if idx == vocab.word2idx['<end>']:\n",
    "                    break\n",
    "                if idx not in [vocab.word2idx['<pad>'], vocab.word2idx['<start>']]:\n",
    "                    ground_truth.append(vocab.idx2word[idx])\n",
    "            \n",
    "            ground_truth = ' '.join(ground_truth)\n",
    "            \n",
    "            # Сохраняем результаты\n",
    "            samples.append({\n",
    "                'features': features.cpu(),\n",
    "                'predicted': predicted_caption,\n",
    "                'ground_truth': ground_truth,\n",
    "                'attention': attention_weights\n",
    "            })\n",
    "            \n",
    "            count += 1\n",
    "    \n",
    "    # Создаем график для визуализации\n",
    "    fig, axes = plt.subplots(num_samples, 1, figsize=(15, 5 * num_samples))\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = axes[i] if num_samples > 1 else axes\n",
    "        \n",
    "        # Отображаем предсказанную и эталонную подпись\n",
    "        ax.text(0.5, 0.5, \n",
    "                f\"Предсказание: {sample['predicted']}\\nЭталон: {sample['ground_truth']}\", \n",
    "                fontsize=12, ha='center')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_attention_patterns(attention_weights, caption):\n",
    "    \"\"\"\n",
    "    Анализ паттернов внимания для понимания работы модели\n",
    "    \n",
    "    Аргументы:\n",
    "        attention_weights: Веса внимания\n",
    "        caption: Сгенерированная подпись\n",
    "    \n",
    "    Анализирует:\n",
    "    1. На какие кадры смотрит модель при генерации каждого слова\n",
    "    2. Какие кадры наиболее важны для всей подписи\n",
    "    3. Визуализирует распределение внимания\n",
    "    \"\"\"\n",
    "    words = caption.split()\n",
    "    \n",
    "    # Создаем матрицу весов внимания\n",
    "    attention_matrix = np.vstack([weights.reshape(-1) for weights in attention_weights])\n",
    "    \n",
    "    # Тепловая карта внимания\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(attention_matrix, aspect='auto', cmap='hot')\n",
    "    plt.colorbar(label='Вес внимания')\n",
    "    plt.xlabel('Позиция кадра')\n",
    "    plt.ylabel('Позиция слова')\n",
    "    plt.yticks(range(len(words)), words)\n",
    "    plt.title('Распределение внимания')\n",
    "    plt.show()\n",
    "    \n",
    "    # Анализ соответствия слов и кадров\n",
    "    word_to_frame_map = attention_matrix.argmax(axis=1)\n",
    "    \n",
    "    print(\"Соответствие слов и кадров:\")\n",
    "    for i, word in enumerate(words):\n",
    "        max_frame = word_to_frame_map[i]\n",
    "        max_weight = attention_matrix[i, max_frame]\n",
    "        print(f\"Слово '{word}' максимально связано с кадром {max_frame} (вес {max_weight:.4f})\")\n",
    "    \n",
    "    # Находим наиболее важные кадры\n",
    "    frame_importance = attention_matrix.sum(axis=0)\n",
    "    most_important_frames = frame_importance.argsort()[-5:][::-1]\n",
    "    \n",
    "    print(\"\\nСамые важные кадры:\")\n",
    "    for frame_idx in most_important_frames:\n",
    "        print(f\"Кадр {frame_idx}: Общий вес внимания {frame_importance[frame_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72288e5a-644f-4bc5-9480-2759e182822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Полный цикл обучения модели\n",
    "\n",
    "def main():\n",
    "    print(\"Подготовка данных...\")\n",
    "    # Пути к данным\n",
    "    video_dir = \"D:\\\\video_to_text\\\\YouTubeClips\"  # Директория с видеофайлами\n",
    "    caption_file = \"D:\\\\video_to_text\\\\AllVideoDescriptions.txt\"  # Файл с текстовыми описаниями\n",
    "    feature_dir = \"D:\\\\video_to_text\\\\features\"  # Директория для сохранения предвычисленных признаков\n",
    "\n",
    "    # Проверка наличия папки с признаками\n",
    "    video_files = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi'))]\n",
    "    features_exist = all(os.path.exists(os.path.join(feature_dir, os.path.splitext(f)[0] + \".npy\")) for f in video_files)\n",
    "    if not features_exist:\n",
    "        print(\"Предварительный расчет признаков...\")\n",
    "        precompute_features(video_dir, feature_dir)\n",
    "    else:\n",
    "        print(\"Все признаки уже вычислены, пропускаем этап извлечения.\")\n",
    "       \n",
    "    # 1. Предварительный расчет признаков из видео\n",
    "    print(\"Предварительный расчет признаков...\")\n",
    "    precompute_features(video_dir, feature_dir)  # Извлечение и сохранение визуальных признаков\n",
    "    \n",
    "    # 2. Построение словаря из текстовых описаний\n",
    "    print(\"Построение словаря...\")\n",
    "    vocab = build_vocab(caption_file, threshold=4)  # Создание словаря (порог вхождения слов = 4)\n",
    "    vocab_size = len(vocab)  # Размер словаря\n",
    "    print(f\"Размер словаря: {vocab_size}\")\n",
    "    \n",
    "    # 3. Создание и разделение датасета\n",
    "    full_dataset = VideoDataset(feature_dir, caption_file, vocab)  # Инициализация набора данных\n",
    "    train_size = int(0.7 * len(full_dataset))  # 70% данных для обучения\n",
    "    val_size = int(0.15 * len(full_dataset))   # 15% для валидации\n",
    "    test_size = len(full_dataset) - train_size - val_size  # Остаток для тестирования\n",
    "    # Разделение данных с фиксированным seed для воспроизводимости\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # 4. Подбор оптимального размера батча\n",
    "    sample_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=caption_collate_fn)  # Тестовый загрузчик данных\n",
    "    sample_data = next(iter(sample_loader))  # Получение одного примера данных\n",
    "    # Инициализация модели для тестирования\n",
    "    model = VideoCaptioningModel(vocab_size=vocab_size, feature_dim=2048, embed_dim=512, \n",
    "                                 encoder_dim=512, decoder_dim=512, attention_dim=512, dropout=0.5).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()  # Функция потерь\n",
    "    # Автоподбор размера батча (макс = 16)\n",
    "    batch_size = find_optimal_batch_size(model, sample_data, criterion, max_batch_size=16)\n",
    "    \n",
    "    # 5. Создание загрузчиков данных с оптимальным размером батча\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=caption_collate_fn)  # Для обучения\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=caption_collate_fn)     # Для валидации\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=caption_collate_fn)            # Для тестирования\n",
    "    \n",
    "    # 6. Инициализация компонентов обучения\n",
    "    model = VideoCaptioningModel(vocab_size=vocab_size, feature_dim=2048, embed_dim=512, \n",
    "                                 encoder_dim=512, decoder_dim=512, attention_dim=512, dropout=0.5).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()  # Функция потерь для классификации\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)  # Оптимизатор с L2-регуляризацией\n",
    "    \n",
    "    # 7. Процесс обучения модели\n",
    "    print(\"Обучение модели...\")\n",
    "    # Обучение в течение 20 эпох\n",
    "    model, history = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20, device=device)\n",
    "    \n",
    "    # 8. Визуализация процесса обучения\n",
    "    plt.plot(history['train_loss'], label='Потери на обучении')  # Потери на обучении\n",
    "    plt.plot(history['val_loss'], label='Потери на валидации')   # Потери на валидации\n",
    "    plt.xlabel('Эпоха')  # Ось X - эпохи\n",
    "    plt.ylabel('Потери') # Ось Y - значение потерь\n",
    "    plt.legend()\n",
    "    plt.title('Прогресс обучения')\n",
    "    plt.grid(True)       # Включение сетки\n",
    "    plt.show()           # Отображение графика\n",
    "    \n",
    "    # 9. Тестирование обученной модели\n",
    "    print(\"Тестирование модели...\")\n",
    "    sample_features, sample_caption, sample_lengths = next(iter(test_loader))  # Получение тестового примера\n",
    "    sample_features = sample_features.to(device)  # Перенос данных на устройство (GPU/CPU)\n",
    "    caption, _ = predict_caption(model, sample_features, vocab)  # Генерация описания\n",
    "    print(f\"Сгенерированное описание: {caption}\")  # Вывод результата\n",
    "\n",
    "    # 10. Визуализация предсказаний для нескольких примеров\n",
    "    print(\"Визуализация предсказаний...\")\n",
    "    visualize_predictions(model, test_loader, vocab, num_samples=3)  # Показать 3 примера\n",
    "    \n",
    "    # 11. Анализ паттернов внимания для сгенерированной подписи\n",
    "    print(\"Анализ паттернов внимания...\")\n",
    "    analyze_attention_patterns(attention_weights, caption)  # Анализ весов внимания\n",
    "    \n",
    "    # Можно также извлечь кадры из видео для визуализации внимания с кадрами\n",
    "\n",
    "    video_id = os.path.splitext(os.listdir(video_dir)[0])[0]\n",
    "    video_path = os.path.join(video_dir, f\"{video_id}.mp4\")\n",
    "    frames = extract_frames(video_path)\n",
    "    visualize_attention(frames, caption, attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c2277-ae45-4fa9-8cf9-a82ec7e3a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d157da-a10f-4fd3-8545-c369b1a94514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
