{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd822a-f08e-4a06-b339-50dca2a126cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Импортируем необходимые библиотеки\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e6cff-bb66-4c70-aeed-80ec45cc7159",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Установим случайные начальные значения для воспроизводимости\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aca0c3-cea0-4be4-b289-aa05a689e106",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Проверим доступность GPU видеокарты\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используемое устройство: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131e7c8-d950-4c7a-950e-ba2fdcea7e39",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1. Предварительная обработка данных\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    \"\"\"Датасет для работы с видео и их описаниями\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dir, caption_file, vocab, max_frames=40):\n",
    "        \"\"\"Инициализация датасета\n",
    "        \n",
    "        Аргументы:\n",
    "            feature_dir (str): Путь к директории с предвычисленными признаками\n",
    "            caption_file (str): Файл с описаниями в формате \"video_id описание\"\n",
    "            vocab (Vocabulary): Объект словаря для токенизации\n",
    "            max_frames (int): Макс. количество кадров на видео\n",
    "        \"\"\"\n",
    "        self.feature_dir = feature_dir\n",
    "        self.max_frames = max_frames\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # Загрузка и парсинг описаний\n",
    "        self.captions = self._load_captions(caption_file)\n",
    "        self.video_ids = list(self.captions.keys())\n",
    "    \n",
    "    def _load_captions(self, caption_file):\n",
    "        \"\"\"Загружает описания из файла\"\"\"\n",
    "        captions = {}\n",
    "        with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(' ', 1)\n",
    "                if len(parts) == 2:\n",
    "                    video_id, caption = parts\n",
    "                    captions.setdefault(video_id, []).append(caption)\n",
    "        return captions\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Получает один элемент датасета по индексу\"\"\"\n",
    "        video_id = self.video_ids[idx]\n",
    "        \n",
    "        # Загрузка предвычисленных признаков\n",
    "        features = self._load_features(video_id)\n",
    "        \n",
    "        # Выбор случайного описания и токенизация\n",
    "        caption = self._process_caption(video_id)\n",
    "        \n",
    "        return features, caption\n",
    "    \n",
    "    def _load_features(self, video_id):\n",
    "        \"\"\"Загружает признаки видео из файла\"\"\"\n",
    "        feature_path = os.path.join(self.feature_dir, f\"{video_id}.npy\")\n",
    "        features = np.load(feature_path)\n",
    "        \n",
    "        # Проверяем и корректируем размерность\n",
    "        if features.ndim > 2:            \n",
    "            features = features.reshape(features.shape[0], -1) # Преобразуем к [seq_len, feature_dim]\n",
    "        return torch.FloatTensor(features).to(device)\n",
    "    \n",
    "    def _process_caption(self, video_id):\n",
    "        \"\"\"Токенизирует и преобразует описание в тензор\"\"\"\n",
    "        caption = np.random.choice(self.captions[video_id])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        caption = [self.vocab('<start>')] + [self.vocab(token) for token in tokens] + [self.vocab('<end>')]\n",
    "        return torch.LongTensor(caption)\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Словарь для преобразования слов в индексы\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "        self._add_special_tokens()\n",
    "    \n",
    "    def _add_special_tokens(self):\n",
    "        \"\"\"Добавляет специальные токены\"\"\"\n",
    "        for token in ['<pad>', '<start>', '<end>', '<unk>']:\n",
    "            self.add_word(token)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        \"\"\"Добавляет слово в словарь\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __call__(self, word):\n",
    "        \"\"\"Возвращает индекс слова или токен <unk>\"\"\"\n",
    "        return self.word2idx.get(word, self.word2idx['<unk>'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(caption_file, threshold=3):\n",
    "    \"\"\"Строит словарь на основе файла с описаниями\"\"\"\n",
    "    counter = Counter()\n",
    "    \n",
    "    # Подсчет частот слов\n",
    "    with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(' ', 1)\n",
    "            if len(parts) == 2:\n",
    "                counter.update(nltk.tokenize.word_tokenize(parts[1].lower()))\n",
    "    \n",
    "    # Фильтрация по порогу\n",
    "    vocab = Vocabulary()\n",
    "    for word, count in counter.items():\n",
    "        if count >= threshold:\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def precompute_features(video_dir, output_dir, batch_size=16):\n",
    "    \"\"\"Предварительно вычисляет признаки видео с помощью ResNet152\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Инициализация модели для извлечения признаков\n",
    "    model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "    feature_extractor = nn.Sequential(*list(model.children())[:-1]).to(device)\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    # Трансформации для кадров\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Обработка видеофайлов\n",
    "    video_files = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi'))]\n",
    "    \n",
    "    for i in range(0, len(video_files), batch_size):\n",
    "        batch_files = video_files[i:i + batch_size]\n",
    "        \n",
    "        for video_file in tqdm(batch_files, desc=f\"Batch {i//batch_size + 1}\"):\n",
    "            \n",
    "            video_id = os.path.splitext(video_file)[0]\n",
    "            feature_path = os.path.join(output_dir, f\"{video_id}.npy\") # Проверяем, существуют ли уже признаки\n",
    "            if os.path.exists(feature_path):\n",
    "                continue  # Признаки уже есть, пропускаем\n",
    "\n",
    "            video_path = os.path.join(video_dir, video_file)\n",
    "            # Извлечение кадров\n",
    "            frames = extract_frames(video_path)\n",
    "            \n",
    "            # Извлечение признаков\n",
    "            features = []\n",
    "            with torch.no_grad():\n",
    "                for frame in frames:\n",
    "                    if frame.ndim == 3:  # Проверка валидности кадра\n",
    "                        frame = transform(frame).unsqueeze(0).to(device)\n",
    "                        feature = feature_extractor(frame).squeeze().cpu().numpy()\n",
    "                        features.append(feature)\n",
    "            \n",
    "            # Сохранение признаков\n",
    "            features = np.stack(features, axis=0)\n",
    "            np.save(os.path.join(output_dir, f\"{video_id}.npy\"), features)\n",
    "\n",
    "def extract_frames(video_path, max_frames=40):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video: {video_path}\")\n",
    "    \n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    sample_rate = max(1, frame_count // max_frames)\n",
    "    frames = []\n",
    "    \n",
    "    for i in range(0, frame_count, sample_rate):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        if len(frames) >= max_frames:\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Если кадров меньше, чем нужно, дублируем последний кадр вместо нулей\n",
    "    if len(frames) < max_frames and len(frames) > 0:\n",
    "        last_frame = frames[-1]\n",
    "        frames.extend([last_frame for _ in range(max_frames - len(frames))])\n",
    "    elif len(frames) == 0:  # Если кадров нет вообще\n",
    "        frames = [np.zeros((224, 224, 3), dtype=np.uint8) for _ in range(max_frames)]\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519709e0-af8e-4d91-9d79-5674870c567d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2. Извлечение признаков\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Класс для извлечения признаков из видеокадров с использованием предобученной CNN\n",
    "    Основные функции:\n",
    "    - Инициализация предобученной модели CNN (по умолчанию ResNet152)\n",
    "    - Преобразование входных кадров к нужному формату\n",
    "    - Извлечение признаков из каждого кадра\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn_model=None):\n",
    "        if cnn_model is None:\n",
    "            # Загрузка предобученной ResNet152\n",
    "            cnn_model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "            # Удаляем последний классификационный слой\n",
    "            self.model = nn.Sequential(*list(cnn_model.children())[:-1])\n",
    "        else:\n",
    "            self.model = cnn_model\n",
    "\n",
    "        # Перенос модели на устройство (GPU/CPU) и перевод в режим оценки\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Определение преобразований для входных изображений\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),               # Конвертация в PIL Image\n",
    "            transforms.Resize((224, 224)),         # Изменение размера под вход сети\n",
    "            transforms.ToTensor(),                 # Конвертация в тензор\n",
    "            transforms.Normalize(                  # Нормализация\n",
    "                mean=[0.485, 0.456, 0.406],        # Средние значения ImageNet\n",
    "                std=[0.229, 0.224, 0.225]          # Стандартные отклонения ImageNet\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def extract_features(self, frames):\n",
    "        \"\"\"\n",
    "        Извлекает признаки из списка кадров\n",
    "        \n",
    "        Аргументы:\n",
    "            frames (list): Список кадров в формате numpy arrays\n",
    "            \n",
    "        Возвращает:\n",
    "            torch.Tensor: Извлеченные признаки размерности [число_кадров, размерность_признака]\n",
    "            \n",
    "        Процесс работы:\n",
    "        1. Применение преобразований к каждому кадру\n",
    "        2. Извлечение признаков с помощью CNN\n",
    "        3. Накопление и объединение признаков\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        with torch.no_grad(): # Отключаем вычисление градиентов для ускорения\n",
    "            for frame in frames:\n",
    "                # Применяем преобразования и добавляем batch-размерность\n",
    "                frame = self.transform(frame).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Извлекаем признаки\n",
    "                feature = self.model(frame)\n",
    "                feature = feature.squeeze() # Удаляем лишние размерности\n",
    "                \n",
    "                features.append(feature.cpu()) # Переносим на CPU для экономии памяти\n",
    "        \n",
    "        return torch.stack(features) # Объединяем все признаки в один тензор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6fb4d-664f-44ce-92bb-78265e21c577",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3. Архитектура модели\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Видео-энкодер для обработки признаков кадров с учетом временной информации\n",
    "    Использует двунаправленный LSTM для анализа последовательности кадров\n",
    "    \n",
    "    Основные функции:\n",
    "    - Обработка признаков отдельных кадров\n",
    "    - Учет временных зависимостей между кадрами\n",
    "    - Подготовка скрытых состояний для декодера\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, hidden_dim, num_layers=1, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Инициализация энкодера\n",
    "        \n",
    "        Аргументы:\n",
    "            feature_dim (int): Размерность входных признаков кадра\n",
    "            hidden_dim (int): Размерность скрытого слоя LSTM\n",
    "            num_layers (int): Количество слоев LSTM\n",
    "            dropout (float): Вероятность дропаута\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Двунаправленный LSTM для временного кодирования\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feature_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,          # Первая размерность - batch\n",
    "            bidirectional=True,       # Двунаправленная архитектура\n",
    "            dropout=dropout if num_layers > 1 else 0  # Дропаут только для многослойных LSTM\n",
    "        )\n",
    "        \n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Прямой проход через энкодер\n",
    "        \n",
    "        Аргументы:\n",
    "            features (torch.Tensor): Признаки видеокадров размерности \n",
    "                                    [batch_size, seq_len, feature_dim]\n",
    "            \n",
    "        Возвращает:\n",
    "            tuple: (outputs, hidden)\n",
    "                - outputs: Выходы LSTM [batch_size, seq_len, hidden_dim*2]\n",
    "                - hidden: Кортеж (скрытое состояние, состояние ячейки)\n",
    "        \"\"\"\n",
    "         # Проверяем размерность входных данных\n",
    "        if features.dim() > 3:\n",
    "            batch_size, seq_len = features.size(0), features.size(1)\n",
    "            # Преобразуем к [batch_size, seq_len, feature_dim]\n",
    "            features = features.view(batch_size, seq_len, -1)\n",
    "            \n",
    "        # Убеждаемся, что последнее измерение имеет правильный размер\n",
    "        if features.size(-1) != self.feature_dim:\n",
    "            raise ValueError(f\"Неверная размерность признаков: ожидается {self.feature_dim}, получено {features.size(-1)}\")\n",
    "        \n",
    "        # Пропускаем признаки через LSTM\n",
    "        outputs, hidden = self.lstm(features)\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Слой внимания для выделения наиболее релевантных частей видео\n",
    "    Реализует механизм внимания на основе скалярного произведения\n",
    "    \n",
    "    Основные функции:\n",
    "    - Вычисление весов внимания для каждого кадра\n",
    "    - Создание контекстного вектора\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_dim, decoder_dim):\n",
    "        \"\"\"\n",
    "        Инициализация слоя внимания\n",
    "        \n",
    "        Аргументы:\n",
    "            encoder_dim (int): Размерность выхода энкодера\n",
    "            decoder_dim (int): Размерность скрытого состояния декодера\n",
    "        \"\"\"\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        # Линейные преобразования для вычисления внимания        \n",
    "        self.encoder_attn = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.full_attn = nn.Linear(decoder_dim, 1)\n",
    "        \n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Прямой проход через слой внимания\n",
    "        \n",
    "        Аргументы:\n",
    "            encoder_outputs (torch.Tensor): Выходы энкодера [batch_size, seq_len, encoder_dim]\n",
    "            decoder_hidden (torch.Tensor): Скрытое состояние декодера [batch_size, decoder_dim]\n",
    "            \n",
    "        Возвращает:\n",
    "            tuple: (context, attention_weights)\n",
    "                - context: Контекстный вектор [batch_size, encoder_dim]\n",
    "                - attention_weights: Веса внимания [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        # Проецируем выходы энкодера в пространство декодера\n",
    "        # Размерность: [batch_size, seq_len, decoder_dim]\n",
    "        attn_proj = self.encoder_attn(encoder_outputs)\n",
    "        \n",
    "        # Добавляем размерность для совместимости\n",
    "        # Размерность: [batch_size, 1, decoder_dim]\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1)\n",
    "        \n",
    "        # Вычисляем оценки внимания через тангенс\n",
    "        # Размерность: [batch_size, seq_len, 1]\n",
    "        attn_scores = self.full_attn(torch.tanh(attn_proj + decoder_hidden))\n",
    "        \n",
    "        # Нормализуем оценки в веса с помощью softmax\n",
    "        # Размерность: [batch_size, seq_len]\n",
    "        attn_weights = F.softmax(attn_scores.squeeze(2), dim=1)\n",
    "        \n",
    "        # Вычисляем взвешенную сумму выходов энкодера\n",
    "        # Размерность: [batch_size, encoder_dim]\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        \n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Декодер подписей, генерирующий описание слово за словом\n",
    "    Использует механизм внимания для фокусировки на релевантных частях видео\n",
    "    \n",
    "    Основные компоненты:\n",
    "    - Слой эмбеддинга слов\n",
    "    - Механизм внимания\n",
    "    - LSTM ячейка\n",
    "    - Выходной слой\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, encoder_dim, hidden_dim, attention_dim, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Инициализация декодера\n",
    "        \n",
    "        Аргументы:\n",
    "            vocab_size (int): Размер словаря\n",
    "            embed_dim (int): Размерность эмбеддинга слов\n",
    "            encoder_dim (int): Размерность выхода энкодера\n",
    "            hidden_dim (int): Размерность скрытого слоя LSTM\n",
    "            attention_dim (int): Размерность механизма внимания\n",
    "            dropout (float): Вероятность дропаута\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Слой эмбеддинга слов\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Слой внимания\n",
    "        self.attention = AttentionLayer(encoder_dim, hidden_dim)\n",
    "        \n",
    "        # LSTM ячейка\n",
    "        self.lstm = nn.LSTMCell(embed_dim + encoder_dim, hidden_dim)\n",
    "        \n",
    "        # Слой дропаута\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Выходной полносвязный слой\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Инициализация весов\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Инициализация весов для эмбеддингов и выходного слоя.\"\"\"\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)  # Равномерная инициализация\n",
    "        self.fc.bias.data.fill_(0)                      # Нулевые смещения\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)         # Равномерная инициализация\n",
    "        \n",
    "    def init_hidden_state(self, batch_size):\n",
    "        \"\"\"\n",
    "        Инициализация скрытого состояния LSTM\n",
    "        \n",
    "        Аргументы:\n",
    "            batch_size (int): Размер батча\n",
    "            \n",
    "        Возвращает:\n",
    "            tuple: (скрытое состояние, состояние ячейки)\n",
    "        \"\"\"\n",
    "        h = torch.zeros(batch_size, self.hidden_dim).to(device)\n",
    "        c = torch.zeros(batch_size, self.hidden_dim).to(device)\n",
    "        return h, c\n",
    "        \n",
    "    def forward(self, encoder_outputs, captions, lengths):\n",
    "        \"\"\"\n",
    "        Прямой проход через декодер (обучение)\n",
    "        \n",
    "        Аргументы:\n",
    "            encoder_outputs (torch.Tensor): Выходы энкодера [batch_size, seq_len, encoder_dim]\n",
    "            captions (torch.Tensor): Истинные подписи [batch_size, max_caption_length]\n",
    "            lengths (list): Фактические длины подписей\n",
    "            \n",
    "        Возвращает:\n",
    "            torch.Tensor: Предсказания [batch_size, max_caption_length, vocab_size]\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        \n",
    "        # Сортируем данные по убыванию длины (для эффективности)\n",
    "        lengths, sort_idx = torch.sort(lengths, descending=True)\n",
    "        encoder_outputs = encoder_outputs[sort_idx]\n",
    "        captions = captions[sort_idx]\n",
    "        \n",
    "        # Инициализируем состояние LSTM\n",
    "        h, c = self.init_hidden_state(batch_size)\n",
    "        \n",
    "        # Определяем максимальную длину в батче\n",
    "        max_length = lengths[0].item()\n",
    "        \n",
    "        # Инициализируем тензор предсказаний\n",
    "        predictions = torch.zeros(batch_size, max_length, self.vocab_size).to(device)\n",
    "        \n",
    "        # Получаем эмбеддинги слов\n",
    "        embeddings = self.embedding(captions)\n",
    "        \n",
    "        # Инициализируем контекстный вектор\n",
    "        context, _ = self.attention(encoder_outputs, h)\n",
    "        \n",
    "        # Генерируем слова последовательно\n",
    "        for t in range(max_length):\n",
    "            # Объединяем эмбеддинг и контекстный вектор\n",
    "            lstm_input = torch.cat([embeddings[:, t], context], dim=1)\n",
    "            \n",
    "            # Прямой проход через LSTM\n",
    "            h, c = self.lstm(lstm_input, (h, c))\n",
    "            \n",
    "            # Вычисляем новый контекст\n",
    "            context, _ = self.attention(encoder_outputs, h)\n",
    "            \n",
    "            # Генерируем предсказание следующего слова\n",
    "            output = self.fc(self.dropout(h))\n",
    "            predictions[:, t] = output\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    # алгоритм лучевого поиска (beam search) для генерации последовательности\n",
    "    def beam_search(self, encoder_outputs, vocab, max_length=20, beam_size=5):\n",
    "        # Получаем размер батча из выходных данных энкодера\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        # Определяем устройство (CPU/GPU) для создания новых тензоров\n",
    "        device = encoder_outputs.device\n",
    "    \n",
    "        # Инициализация скрытых состояний LSTM декодера\n",
    "        h, c = self.init_hidden_state(batch_size)\n",
    "        \n",
    "        # Получаем индексы служебных токенов\n",
    "        start_idx = vocab.word2idx['<start>']  # Токен начала последовательности\n",
    "        end_idx = vocab.word2idx['<end>']      # Токен конца последовательности\n",
    "    \n",
    "        # Инициализируем список \"лучей\" (кандидатов)\n",
    "        # Каждый луч: (суммарный логарифм вероятности, последовательность, h, c)\n",
    "        beams = [(0.0, [start_idx], h, c)]\n",
    "        completed = []  # Завершенные последовательности\n",
    "    \n",
    "        # Основной цикл генерации по шагам\n",
    "        for _step in range(max_length):\n",
    "            all_candidates = []\n",
    "            \n",
    "            # Обрабатываем каждый текущий луч\n",
    "            for score, seq, h, c in beams:\n",
    "                # Проверка завершения последовательности\n",
    "                if seq[-1] == end_idx:\n",
    "                    completed.append((score, seq))\n",
    "                    continue  # Пропускаем завершенные последовательности\n",
    "    \n",
    "                # Подготовка последнего токена\n",
    "                last = torch.LongTensor([seq[-1]]).to(device)\n",
    "                \n",
    "                # Forward pass через модель\n",
    "                emb = self.embedding(last)          # Получаем эмбеддинг токена [1, E]\n",
    "                context, _ = self.attention(encoder_outputs, h)  # Контекст внимания\n",
    "                lstm_input = torch.cat([emb, context], dim=1)     # Объединение входов\n",
    "                h_new, c_new = self.lstm(lstm_input, (h, c))      # Обновление состояний\n",
    "                out = self.fc(h_new)                # Преобразование в пространство слов\n",
    "                logp = F.log_softmax(out, dim=1)    # Логарифмические вероятности [1, V]\n",
    "    \n",
    "                # Выбор топ-k кандидатов\n",
    "                topk_logp, topk_idx = logp.topk(beam_size, dim=1)\n",
    "                \n",
    "                # Генерация новых кандидатов для текущего луча\n",
    "                for k in range(beam_size):\n",
    "                    next_token = topk_idx[0, k].item()    # Индекс следующего токена\n",
    "                    next_score = topk_logp[0, k].item()   # Логарифм вероятности\n",
    "                    \n",
    "                    # Обновляем параметры для нового кандидата:\n",
    "                    new_score = score + next_score        # Накопленный счет\n",
    "                    new_seq = seq + [next_token]         # Расширенная последовательность\n",
    "                    all_candidates.append((new_score, new_seq, h_new.clone(), c_new.clone()))\n",
    "    \n",
    "            # Выходим, если нет кандидатов для продолжения\n",
    "            if not all_candidates:\n",
    "                break\n",
    "    \n",
    "            # Сортируем всех кандидатов по убыванию суммарного счета\n",
    "            all_candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "            # Отбираем топ-beam_size кандидатов\n",
    "            beams = all_candidates[:beam_size]\n",
    "    \n",
    "            # Проверка раннего завершения (все лучи завершены)\n",
    "            if all(b[1][-1] == end_idx for b in beams):\n",
    "                completed.extend(beams)\n",
    "                break\n",
    "    \n",
    "        # Обработка случая, когда ни один луч не был завершен\n",
    "        if not completed:\n",
    "            completed = beams\n",
    "    \n",
    "        # Выбор лучшей последовательности\n",
    "        completed.sort(key=lambda x: x[0], reverse=True)\n",
    "        best_seq = completed[0][1]\n",
    "    \n",
    "        # Постобработка последовательности:\n",
    "        # 1. Удаляем стартовый токен\n",
    "        if best_seq and best_seq[0] == start_idx:\n",
    "            best_seq = best_seq[1:]\n",
    "        \n",
    "        # 2. Обрезаем после первого end токена (не включая его)\n",
    "        if end_idx in best_seq:\n",
    "            cut = best_seq.index(end_idx)\n",
    "            best_seq = best_seq[:cut]\n",
    "        \n",
    "        # 3. Удаляем любые оставшиеся end токены (защита)\n",
    "        best_seq = [tok for tok in best_seq if tok != end_idx]\n",
    "    \n",
    "        # Возвращаем последовательность как тензор\n",
    "        return torch.LongTensor(best_seq).to(device)\n",
    "\n",
    "\n",
    "class VideoCaptioningModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Полная модель генерации подписей к видео, объединяющая:\n",
    "    - Энкодер видео\n",
    "    - Декодер с механизмом внимания\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, feature_dim=2048, embed_dim=512, encoder_dim=512, \n",
    "                 decoder_dim=512, attention_dim=512, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Инициализация модели\n",
    "        \n",
    "        Аргументы:\n",
    "            vocab_size (int): Размер словаря\n",
    "            feature_dim (int): Размерность входных признаков\n",
    "            embed_dim (int): Размерность эмбеддингов слов\n",
    "            encoder_dim (int): Размерность скрытого слоя энкодера\n",
    "            decoder_dim (int): Размерность скрытого слоя декодера\n",
    "            attention_dim (int): Размерность механизма внимания\n",
    "            dropout (float): Вероятность дропаута\n",
    "        \"\"\"\n",
    "        super(VideoCaptioningModel, self).__init__()\n",
    "        \n",
    "        # Инициализация энкодера\n",
    "        self.encoder = Encoder(\n",
    "            feature_dim=feature_dim,\n",
    "            hidden_dim=encoder_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Инициализация декодера\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            encoder_dim=encoder_dim * 2,  # Учитываем двунаправленность энкодера\n",
    "            hidden_dim=decoder_dim,\n",
    "            attention_dim=attention_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"\n",
    "        Прямой проход через модель\n",
    "        \n",
    "        Аргументы:\n",
    "            features (torch.Tensor): Признаки видео [batch_size, seq_len, feature_dim]\n",
    "            captions (torch.Tensor): Истинные подписи [batch_size, max_caption_length]\n",
    "            lengths (list): Фактические длины подписей\n",
    "            \n",
    "        Возвращает:\n",
    "            torch.Tensor: Предсказания модели [batch_size, max_caption_length, vocab_size]\n",
    "        \"\"\"\n",
    "        # Кодируем видео\n",
    "        encoder_outputs, _ = self.encoder(features)\n",
    "        \n",
    "        # Декодируем подписи\n",
    "        outputs = self.decoder(encoder_outputs, captions, lengths)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da612b-c046-41b0-ad08-7da38bb361c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 4. Функции обучения модели\n",
    "\n",
    "def train_epoch(train_loader, model, criterion, optimizer,\n",
    "                device, teacher_forcing_ratio):\n",
    "    model.train()  # Переводим модель в режим обучения\n",
    "    epoch_loss = 0.0  # Инициализация лосса для эпохи\n",
    "\n",
    "    # Итерация по батчам данных\n",
    "    for features, captions, lengths in train_loader:\n",
    "        # Перенос данных на вычислительное устройство (GPU/CPU)\n",
    "        features, captions = features.to(device), captions.to(device)\n",
    "        optimizer.zero_grad()  # Обнуление градиентов\n",
    "\n",
    "        # --- Прямой проход через энкодер ---\n",
    "        encoder_outputs, _ = model.encoder(features)  # [B, H], где H - размер скрытого состояния\n",
    "        batch_size = features.size(0)\n",
    "        \n",
    "        # Инициализация скрытых состояний декодера\n",
    "        h, c = model.decoder.init_hidden_state(batch_size)\n",
    "        \n",
    "        # Начальный токен - <start> для всех примеров в батче\n",
    "        inputs = captions[:, 0]  # [B]\n",
    "        max_len = max(lengths)   # Максимальная длина подписи в текущем батче\n",
    "        loss = 0.0  # Лосс для текущего батча\n",
    "\n",
    "        # Цикл по временным шагам (пропускаем первый токен <start>)\n",
    "        for t in range(1, max_len):\n",
    "            # a) Эмбеддинг токенов\n",
    "            emb = model.decoder.embedding(inputs)  # [B, embedding_dim]\n",
    "            \n",
    "            # b) Механизм внимания\n",
    "            context, _ = model.decoder.attention(encoder_outputs, h)  # [B, hidden_size]\n",
    "            \n",
    "            # c) Объединение эмбеддинга и контекста\n",
    "            lstm_input = torch.cat([emb, context], dim=1)  # [B, embedding_dim + hidden_size]\n",
    "            \n",
    "            # d) Шаг LSTM декодера\n",
    "            h, c = model.decoder.lstm(lstm_input, (h, c))  # Обновление состояний\n",
    "            \n",
    "            # e) Преобразование скрытого состояния в логиты\n",
    "            logits = model.decoder.fc(h)  # [B, vocab_size]\n",
    "            \n",
    "            # f) Расчет лосса между предсказанием и истинным токеном\n",
    "            loss += criterion(logits, captions[:, t])  # Накопление лосса\n",
    "            \n",
    "            # g) Teacher Forcing: выбор следующего входного токена\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                inputs = captions[:, t]  # Истинный предыдущий токен\n",
    "            else:\n",
    "                inputs = logits.argmax(dim=1)  # Предсказанный токен\n",
    "\n",
    "        # Нормализация лосса по длине последовательности\n",
    "        loss = loss / max_len\n",
    "        # Обратное распространение ошибки\n",
    "        loss.backward()\n",
    "        # Обновление параметров модели\n",
    "        optimizer.step()\n",
    "        # Накопление лосса для эпохи\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Возвращаем средний лосс на батч за эпоху\n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    # Переводим модель в режим оценки (отключаем dropout и другие тренировочные механизмы)\n",
    "    model.eval()\n",
    "    total_loss = 0.0  # Инициализация общего лосса для эпохи валидации\n",
    "\n",
    "    # Блок без вычисления градиентов для экономии памяти и ускорения вычислений\n",
    "    with torch.no_grad():\n",
    "        # Итерация по батчам данных\n",
    "        for features, captions, lengths in dataloader:\n",
    "            # Перенос данных на вычислительное устройство\n",
    "            features, captions = features.to(device), captions.to(device)\n",
    "            \n",
    "            # Прямой проход через энкодер\n",
    "            encoder_outputs, _ = model.encoder(features)  # [B, H]\n",
    "            batch_size = features.size(0)\n",
    "            \n",
    "            # Инициализация скрытых состояний декодера\n",
    "            h, c = model.decoder.init_hidden_state(batch_size)\n",
    "            \n",
    "            # Начальный токен <start> для всех примеров в батче\n",
    "            inputs = captions[:, 0]  # [B]\n",
    "            max_len = max(lengths)   # Максимальная длина в текущем батче\n",
    "            loss = 0.0  # Лосс для текущего батча\n",
    "\n",
    "            # Цикл генерации по временным шагам (без учительского форсинга)\n",
    "            for t in range(1, max_len):\n",
    "                # 1. Получение эмбеддинга токена\n",
    "                emb = model.decoder.embedding(inputs)  # [B, embedding_dim]\n",
    "                \n",
    "                # 2. Вычисление контекста через механизм внимания\n",
    "                context, _ = model.decoder.attention(encoder_outputs, h)  # [B, hidden_size]\n",
    "                \n",
    "                # 3. Объединение эмбеддинга и контекста\n",
    "                lstm_input = torch.cat([emb, context], dim=1)  # [B, emb_dim + hidden_size]\n",
    "                \n",
    "                # 4. Обновление состояний LSTM\n",
    "                h, c = model.decoder.lstm(lstm_input, (h, c))\n",
    "                \n",
    "                # 5. Преобразование в логиты словаря\n",
    "                logits = model.decoder.fc(h)  # [B, vocab_size]\n",
    "                \n",
    "                # 6. Накопление лосса между предсказанием и истинным токеном\n",
    "                loss += criterion(logits, captions[:, t])\n",
    "                \n",
    "                # 7. Всегда используем ground-truth токены (без teacher forcing)\n",
    "                inputs = captions[:, t]  # Принудительная подача правильных токенов\n",
    "\n",
    "            # Усреднение лосса по длине последовательности и добавление к общему лоссу\n",
    "            total_loss += (loss / max_len).item()\n",
    "\n",
    "    # Возвращаем средний лосс на батч за эпоху валидации\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731ec50d-ab0c-4ee8-a3d9-9318702e118c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 5. Основной цикл обучения модели\n",
    "\n",
    "def train_model(model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                num_epochs,\n",
    "                device,\n",
    "                save_dir='checkpoints',\n",
    "                save_every=5,\n",
    "                scheduler=None,\n",
    "                tf_start=1.0,\n",
    "                tf_end=0.5):\n",
    "    \"\"\"\n",
    "    Основная функция для обучения модели с валидацией и сохранением чекпоинтов.\n",
    "\n",
    "    Параметры:\n",
    "        model: nn.Module - модель для обучения\n",
    "        train_loader, val_loader: DataLoader - загрузчики данных\n",
    "        criterion: функция потерь\n",
    "        optimizer: оптимизатор\n",
    "        num_epochs: int - количество эпох обучения\n",
    "        device: torch.device - устройство для вычислений (CPU/GPU)\n",
    "        save_dir: str - директория для сохранения чекпоинтов\n",
    "        save_every: int - периодичность сохранения чекпоинтов (в эпохах)\n",
    "        scheduler: объект - планировщик скорости обучения (ReduceLROnPlateau)\n",
    "        tf_start, tf_end: float - начальное и конечное значение коэффициента Teacher Forcing\n",
    "    \"\"\"\n",
    "    # Создаем директорию для сохранения чекпоинтов\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Инициализируем историю обучения\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    # Основной цикл обучения по эпохам\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # 1) Расчет коэффициента Teacher Forcing для текущей эпохи\n",
    "        # Линейное уменьшение от tf_start до tf_end в течение обучения\n",
    "        tf_ratio = max(\n",
    "            tf_end,  # Минимальное значение\n",
    "            tf_start - (epoch - 1) * (tf_start - tf_end) / (num_epochs - 1)\n",
    "        )\n",
    "\n",
    "        # 2) Обучение на одной эпохе\n",
    "        train_loss = train_epoch(\n",
    "            train_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device,\n",
    "            tf_ratio\n",
    "        )\n",
    "\n",
    "        # 3) Валидация модели\n",
    "        val_loss = validate(\n",
    "            model,\n",
    "            val_loader,\n",
    "            criterion,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        # 4) Обновление скорости обучения (если используется планировщик)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)  # Обычно для ReduceLROnPlateau\n",
    "\n",
    "        # 5) Сохранение метрик и логирование\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']  # Текущая скорость обучения\n",
    "        \n",
    "        # Вывод информации о прогрессе\n",
    "        print(f\"Эпоха {epoch}/{num_epochs}, \"\n",
    "              f\"Ошибка (обучение): {train_loss:.4f}, \"\n",
    "              f\"Ошибка (валидация): {val_loss:.4f}, \"\n",
    "              f\"Скорость обучения: {current_lr:.6f}, \"\n",
    "              f\"Teacher Forcing: {tf_ratio:.3f}\")\n",
    "\n",
    "        # 6) Сохранение промежуточных результатов\n",
    "        if epoch % save_every == 0:\n",
    "            # Формируем словарь с состоянием модели\n",
    "            ckpt = {\n",
    "                'epoch': epoch,\n",
    "                'model_state': model.state_dict(),  # Веса модели\n",
    "                'optim_state': optimizer.state_dict(),  # Состояние оптимизатора\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'lr': current_lr,\n",
    "                'tf_ratio': tf_ratio\n",
    "            }\n",
    "            # Сохраняем чекпоинт\n",
    "            path = os.path.join(save_dir, f\"ckpt_epoch{epoch}.pt\")\n",
    "            torch.save(ckpt, path)\n",
    "            print(f\"Чекпоинт сохранен: {path}\")\n",
    "\n",
    "    # 7) Сохранение финальной модели после завершения обучения\n",
    "    final_path = os.path.join(save_dir, \"final_model.pt\")\n",
    "    torch.save({\n",
    "        'model_state': model.state_dict(),\n",
    "        'history': history  # Сохраняем историю обучения\n",
    "    }, final_path)\n",
    "    print(f\"Финальная модель сохранена: {final_path}\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8398ad-05c1-4470-a243-28a64fe2a9f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 6. Функции оценки качества модели\n",
    "\n",
    "def predict_caption(model, features, vocab, beam_size=3, max_length=20):\n",
    "    \"\"\"\n",
    "    Генерация подписи с использованием обученной модели и сбор весов внимания\n",
    "\n",
    "    Параметры:\n",
    "        model: Обученная модель (ожидается архитектура encoder-decoder с вниманием)\n",
    "        features: Тензор признаков от энкодера [1, num_frames, feat_dim]\n",
    "        vocab: Словарь для преобразования слов<->индексы\n",
    "        beam_size: Параметр для beam search (не используется в текущей реализации)\n",
    "        max_length: Максимальная длина генерируемой подписи\n",
    "\n",
    "    Возвращает:\n",
    "        caption_str: Строка сгенерированной подписи через пробел\n",
    "        attention_weights: Список тензоров с весами внимания для каждого шага\n",
    "    \"\"\"\n",
    "    # Переводим модель в режим инференса\n",
    "    model.eval()\n",
    "    \n",
    "    # Инициализируем список для сбора весов внимания\n",
    "    attention_weights = []\n",
    "    \n",
    "    # Отключаем вычисление градиентов для ускорения и экономии памяти\n",
    "    with torch.no_grad():\n",
    "        # Прямой проход через энкодер\n",
    "        enc_out, _ = model.encoder(features)  # enc_out: [1, num_frames, feat_dim]\n",
    "        \n",
    "        # Инициализируем скрытые состояния декодера\n",
    "        h, c = model.decoder.init_hidden_state(features.size(0))  # batch_size=1\n",
    "        \n",
    "        # Начальный токен <start> как вход для первого шага\n",
    "        inputs = torch.LongTensor([vocab.word2idx['<start>']]).to(features.device)\n",
    "        \n",
    "        generated_idxs = []  # Список для сгенерированных индексов слов\n",
    "\n",
    "        # Цикл генерации по временным шагам\n",
    "        for _ in range(max_length):\n",
    "            # 1. Получаем эмбеддинг текущего слова\n",
    "            emb = model.decoder.embedding(inputs)  # [1, embedding_dim]\n",
    "            \n",
    "            # 2. Вычисляем контекст и веса внимания\n",
    "            context, alpha = model.decoder.attention(enc_out, h)\n",
    "            # alpha: [1, num_frames] - веса внимания для каждого кадра/региона\n",
    "            \n",
    "            # Сохраняем веса внимания (убираем batch-размер)\n",
    "            attention_weights.append(alpha.squeeze(0).cpu())  # [num_frames]\n",
    "            \n",
    "            # 3. Объединяем эмбеддинг и контекст\n",
    "            lstm_input = torch.cat([emb, context], dim=1)  # [1, emb_dim + feat_dim]\n",
    "            \n",
    "            # 4. Обновляем состояние LSTM\n",
    "            h, c = model.decoder.lstm(lstm_input, (h, c))\n",
    "            \n",
    "            # 5. Получаем логиты для следующего слова\n",
    "            logits = model.decoder.fc(h)  # [1, vocab_size]\n",
    "            \n",
    "            # 6. Выбираем наиболее вероятный токен (жадный алгоритм)\n",
    "            inputs = logits.argmax(1)  # [1]\n",
    "            \n",
    "            # Проверяем условие завершения генерации\n",
    "            if inputs.item() == vocab.word2idx['<end>']:\n",
    "                break\n",
    "                \n",
    "            generated_idxs.append(inputs.item())\n",
    "\n",
    "    # Преобразование индексов в слова через словарь\n",
    "    words = [vocab.idx2word[idx] for idx in generated_idxs]\n",
    "    \n",
    "    # Возвращаем результат и историю внимания\n",
    "    return ' '.join(words), attention_weights\n",
    "\n",
    "\n",
    "def caption_collate_fn(batch, vocab):\n",
    "    \"\"\"Пользовательская функция для обработки батчей с подписями разной длины\"\"\"\n",
    "    # Сортируем батч по длине подписей (по убыванию)\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    features, captions = zip(*batch)\n",
    "    \n",
    "    # Объединяем признаки\n",
    "    features = torch.stack(features, 0)\n",
    "    \n",
    "    # Получаем длины подписей\n",
    "    lengths = torch.LongTensor([len(cap) for cap in captions])\n",
    "    \n",
    "    # Определяем максимальную длину для паддинга\n",
    "    max_length = max(lengths)\n",
    "    \n",
    "    # Создаем тензор с заполненными подписями\n",
    "    padded_captions = torch.zeros(len(captions), max_length, dtype=torch.long)\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        padded_captions[i, :end] = cap[:end]\n",
    "        # Добавляем токен <end> в конец последовательности\n",
    "        if end < max_length:\n",
    "            padded_captions[i, end] = vocab.word2idx['<end>']\n",
    "    \n",
    "    return features, padded_captions, lengths\n",
    "\n",
    "\n",
    "def visualize_attention(video_frames, caption, attention_weights, grid_shape=(5, 8)):\n",
    "    \"\"\"\n",
    "    Визуализирует веса внимания модели в виде тепловых карт для каждого слова подписи\n",
    "\n",
    "    Параметры:\n",
    "        video_frames (list): Список кадров видео (не используется в текущей реализации)\n",
    "        caption (str): Сгенерированная текстовая подпись\n",
    "        attention_weights (list): Список тензоров с весами внимания для каждого слова\n",
    "        grid_shape (tuple): Размерность сетки для визуализации (строки, столбцы)\n",
    "    \"\"\"\n",
    "    # Разделяем подпись на отдельные слова\n",
    "    words = caption.split()\n",
    "    \n",
    "    # Проверка согласованности данных\n",
    "    if len(words) != len(attention_weights):\n",
    "        print(f\"Предупреждение: Количество слов ({len(words)}) не совпадает с количеством карт внимания ({len(attention_weights)})\")\n",
    "        return\n",
    "\n",
    "    # Создаем сетку графиков (вертикальное расположение)\n",
    "    rows = len(words)\n",
    "    fig, axes = plt.subplots(rows, 1, figsize=(12, 3*rows))\n",
    "    \n",
    "    # Обрабатываем случай с одной подписью (исправление размерности осей)\n",
    "    if rows == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Генерация тепловых карт для каждого слова\n",
    "    for i, (word, weights) in enumerate(zip(words, attention_weights)):\n",
    "        ax = axes[i]\n",
    "        try:\n",
    "            # 1. Преобразование весов в 2D сетку\n",
    "            attention_grid = weights.numpy().reshape(grid_shape)\n",
    "            \n",
    "            # 2. Отображение тепловой карты\n",
    "            im = ax.imshow(attention_grid, \n",
    "                          cmap='viridis', \n",
    "                          interpolation='nearest')\n",
    "            \n",
    "            # 3. Добавление цветовой шкалы\n",
    "            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            \n",
    "            # 4. Настройка оформления\n",
    "            ax.set_title(f\"Слово: '{word}'\", fontsize=12, pad=10)\n",
    "            ax.axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка визуализации для слова '{word}': {str(e)}\")\n",
    "            ax.axis('off')  # Скрываем поврежденные оси\n",
    "\n",
    "    # Оптимизация расположения элементов\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_metrics(model, dataloader, vocab, device):\n",
    "    \"\"\"Оценка качества модели по BLEU-4\n",
    "    \n",
    "    Аргументы:\n",
    "        model: Обученная модель\n",
    "        dataloader: Загрузчик данных для оценки\n",
    "        vocab: Словарь\n",
    "        device: Устройство вычислений\n",
    "        \n",
    "    Возвращает:\n",
    "        dict: Словарь с метриками качества\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, captions, lengths in tqdm(dataloader, desc='Оценка'):\n",
    "            # Генерация подписи\n",
    "            pred_caption, _ = predict_caption(model, features.to(device), vocab)\n",
    "            hypotheses.append(pred_caption.split())\n",
    "            \n",
    "            # Обработка эталонной подписи\n",
    "            ref = []\n",
    "            for idx in captions[0].cpu().numpy():\n",
    "                if idx == vocab.word2idx['<end>']:\n",
    "                    break\n",
    "                if idx not in [vocab.word2idx['<pad>'], vocab.word2idx['<start>']]:\n",
    "                    ref.append(vocab.idx2word[idx])\n",
    "            references.append([ref])\n",
    "        \n",
    "    # Вычисление метрик\n",
    "    bleu4 = nltk.translate.bleu_score.corpus_bleu(\n",
    "        references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    return {'bleu4': bleu4}\n",
    "\n",
    "\n",
    "def find_optimal_batch_size(model, sample_data, criterion, max_batch_size=16):\n",
    "    \"\"\"Автоматический подбор оптимального размера батча\n",
    "    \n",
    "    Аргументы:\n",
    "        model: Модель для тестирования\n",
    "        sample_data: Пример данных (features, captions)\n",
    "        criterion: Функция потерь для оценки результата\n",
    "        max_batch_size: Максимальный размер батча для проверки\n",
    "        \n",
    "    Возвращает:\n",
    "        int: Оптимальный размер батча\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimal_size = 1\n",
    "    features, captions, lengths = sample_data\n",
    "    \n",
    "    for bs in range(1, max_batch_size + 1, 2):\n",
    "        try:\n",
    "            # Тестовый прогон\n",
    "            batch_features = features.repeat(bs, 1, 1).to(device)\n",
    "            batch_captions = captions.repeat(bs, 1).to(device)\n",
    "            batch_lengths = lengths.repeat(bs) # Создаем batch_lengths\n",
    "\n",
    "            # Передаем все три параметра в модель\n",
    "            outputs = model(batch_features, batch_captions, batch_lengths)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), batch_captions.view(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            optimal_size = bs\n",
    "            print(f\"Батч {bs} успешно обработан\")\n",
    "            \n",
    "            # Очистка памяти\n",
    "            del batch_features, batch_captions, batch_lengths, outputs, loss\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"памяти\" in str(e).lower():\n",
    "                print(f\"Достигнут лимит памяти при батче {bs}\")\n",
    "                break\n",
    "                \n",
    "    print(f\"Оптимальный размер батча: {optimal_size}\")\n",
    "    return optimal_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d783a698-532d-4eb4-a21f-afc3bd2d54f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Визуализация\n",
    "\n",
    "def visualize_predictions(model, dataloader, vocab, num_samples=5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Визуализация предсказаний модели и тепловых карт внимания.\n",
    "\n",
    "    Args:\n",
    "        model: обученная модель\n",
    "        dataloader: загрузчик тестовых данных\n",
    "        vocab: объект Vocabulary\n",
    "        num_samples: сколько примеров показать\n",
    "        device: 'cuda' или 'cpu'\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    samples = []\n",
    "    count = 0\n",
    "\n",
    "    # Собираем несколько примеров\n",
    "    with torch.no_grad():\n",
    "        for features, captions, lengths in dataloader:\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "            features = features.to(device)\n",
    "            pred_caption, attn_weights = predict_caption(model, features, vocab)\n",
    "\n",
    "            # decode ground truth\n",
    "            gt_idxs = captions[0].cpu().tolist()\n",
    "            gt_words = []\n",
    "            for idx in gt_idxs:\n",
    "                if idx == vocab.word2idx['<end>']:\n",
    "                    break\n",
    "                if idx not in {vocab.word2idx['<pad>'],\n",
    "                               vocab.word2idx['<start>']}:\n",
    "                    gt_words.append(vocab.idx2word[idx])\n",
    "            gt_caption = ' '.join(gt_words)\n",
    "\n",
    "            samples.append((gt_caption, pred_caption, attn_weights))\n",
    "            count += 1\n",
    "\n",
    "    # Рисуем для каждого примера текст + heatmap\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(12, 4 * num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "\n",
    "    for i, (gt, pred, attn) in enumerate(samples):\n",
    "        ax_text, ax_hm = axes[i]\n",
    "\n",
    "        # 1) Текстовая часть\n",
    "        ax_text.axis('off')\n",
    "        ax_text.text(0, 0.6, f\"GT:  {gt}\", fontsize=12)\n",
    "        ax_text.text(0, 0.3, f\"PR:  {pred}\", fontsize=12, color='blue')\n",
    "\n",
    "        # 2) Тепловая карта внимания\n",
    "        # Превращаем list[tensor] → numpy matrix\n",
    "        attn_numpy = np.vstack([\n",
    "            w.detach().cpu().reshape(1, -1) for w in attn\n",
    "        ])\n",
    "        im = ax_hm.imshow(attn_numpy,\n",
    "                          aspect='auto',\n",
    "                          cmap='viridis')\n",
    "        ax_hm.set_title(\"Attention Heatmap\")\n",
    "        ax_hm.set_ylabel(\"Шаг слова\")\n",
    "        ax_hm.set_xlabel(\"Позиция кадра\")\n",
    "        # подписи по осям Y - слова предсказания\n",
    "        words = pred.split()\n",
    "        ax_hm.set_yticks(range(len(words)))\n",
    "        ax_hm.set_yticklabels(words, fontsize=10)\n",
    "        plt.colorbar(im, ax=ax_hm, fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
