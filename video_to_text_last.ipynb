{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd822a-f08e-4a06-b339-50dca2a126cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Импортируем необходимые библиотеки\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e6cff-bb66-4c70-aeed-80ec45cc7159",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Установим случайные начальные значения для воспроизводимости\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aca0c3-cea0-4be4-b289-aa05a689e106",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Проверим доступность GPU видеокарты\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используемое устройство: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131e7c8-d950-4c7a-950e-ba2fdcea7e39",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1. Предварительная обработка данных\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    \"\"\"Датасет для работы с видео и их описаниями\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dir, caption_file, vocab, max_frames=40):\n",
    "        \"\"\"Инициализация датасета\n",
    "        \n",
    "        Аргументы:\n",
    "            feature_dir (str): Путь к директории с предвычисленными признаками\n",
    "            caption_file (str): Файл с описаниями в формате \"video_id описание\"\n",
    "            vocab (Vocabulary): Объект словаря для токенизации\n",
    "            max_frames (int): Макс. количество кадров на видео\n",
    "        \"\"\"\n",
    "        self.feature_dir = feature_dir\n",
    "        self.max_frames = max_frames\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # Загрузка и парсинг описаний\n",
    "        self.captions = self._load_captions(caption_file)\n",
    "        self.video_ids = list(self.captions.keys())\n",
    "    \n",
    "    def _load_captions(self, caption_file):\n",
    "        \"\"\"Загружает описания из файла\"\"\"\n",
    "        captions = {}\n",
    "        with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(' ', 1)\n",
    "                if len(parts) == 2:\n",
    "                    video_id, caption = parts\n",
    "                    captions.setdefault(video_id, []).append(caption)\n",
    "        return captions\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Получает один элемент датасета по индексу\"\"\"\n",
    "        video_id = self.video_ids[idx]\n",
    "        \n",
    "        # Загрузка предвычисленных признаков\n",
    "        features = self._load_features(video_id)\n",
    "        \n",
    "        # Выбор случайного описания и токенизация\n",
    "        caption = self._process_caption(video_id)\n",
    "        \n",
    "        return features, caption\n",
    "    \n",
    "    def _load_features(self, video_id):\n",
    "        \"\"\"Загружает признаки видео из файла\"\"\"\n",
    "        feature_path = os.path.join(self.feature_dir, f\"{video_id}.npy\")\n",
    "        features = np.load(feature_path)\n",
    "        \n",
    "        # Проверяем и корректируем размерность\n",
    "        if features.ndim > 2:            \n",
    "            features = features.reshape(features.shape[0], -1) # Преобразуем к [seq_len, feature_dim]\n",
    "        return torch.FloatTensor(features).to(device)\n",
    "    \n",
    "    def _process_caption(self, video_id):\n",
    "        \"\"\"Токенизирует и преобразует описание в тензор\"\"\"\n",
    "        caption = np.random.choice(self.captions[video_id])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        caption = [self.vocab('<start>')] + [self.vocab(token) for token in tokens] + [self.vocab('<end>')]\n",
    "        return torch.LongTensor(caption)\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Словарь для преобразования слов в индексы\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "        self._add_special_tokens()\n",
    "    \n",
    "    def _add_special_tokens(self):\n",
    "        \"\"\"Добавляет специальные токены\"\"\"\n",
    "        for token in ['<pad>', '<start>', '<end>', '<unk>']:\n",
    "            self.add_word(token)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        \"\"\"Добавляет слово в словарь\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __call__(self, word):\n",
    "        \"\"\"Возвращает индекс слова или токен <unk>\"\"\"\n",
    "        return self.word2idx.get(word, self.word2idx['<unk>'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(caption_file, threshold=3):\n",
    "    \"\"\"Строит словарь на основе файла с описаниями\"\"\"\n",
    "    counter = Counter()\n",
    "    \n",
    "    # Подсчет частот слов\n",
    "    with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(' ', 1)\n",
    "            if len(parts) == 2:\n",
    "                counter.update(nltk.tokenize.word_tokenize(parts[1].lower()))\n",
    "    \n",
    "    # Фильтрация по порогу\n",
    "    vocab = Vocabulary()\n",
    "    for word, count in counter.items():\n",
    "        if count >= threshold:\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def precompute_features(video_dir, output_dir, batch_size=16):\n",
    "    \"\"\"Предварительно вычисляет признаки видео с помощью ResNet152\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Инициализация модели для извлечения признаков\n",
    "    model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "    feature_extractor = nn.Sequential(*list(model.children())[:-1]).to(device)\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    # Трансформации для кадров\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Обработка видеофайлов\n",
    "    video_files = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi'))]\n",
    "    \n",
    "    for i in range(0, len(video_files), batch_size):\n",
    "        batch_files = video_files[i:i + batch_size]\n",
    "        \n",
    "        for video_file in tqdm(batch_files, desc=f\"Batch {i//batch_size + 1}\"):\n",
    "            \n",
    "            video_id = os.path.splitext(video_file)[0]\n",
    "            feature_path = os.path.join(output_dir, f\"{video_id}.npy\") # Проверяем, существуют ли уже признаки\n",
    "            if os.path.exists(feature_path):\n",
    "                continue  # Признаки уже есть, пропускаем\n",
    "\n",
    "            video_path = os.path.join(video_dir, video_file)\n",
    "            # Извлечение кадров\n",
    "            frames = extract_frames(video_path)\n",
    "            \n",
    "            # Извлечение признаков\n",
    "            features = []\n",
    "            with torch.no_grad():\n",
    "                for frame in frames:\n",
    "                    if frame.ndim == 3:  # Проверка валидности кадра\n",
    "                        frame = transform(frame).unsqueeze(0).to(device)\n",
    "                        feature = feature_extractor(frame).squeeze().cpu().numpy()\n",
    "                        features.append(feature)\n",
    "            \n",
    "            # Сохранение признаков\n",
    "            features = np.stack(features, axis=0)\n",
    "            np.save(os.path.join(output_dir, f\"{video_id}.npy\"), features)\n",
    "\n",
    "def extract_frames(video_path, max_frames=40):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video: {video_path}\")\n",
    "    \n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    sample_rate = max(1, frame_count // max_frames)\n",
    "    frames = []\n",
    "    \n",
    "    for i in range(0, frame_count, sample_rate):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        if len(frames) >= max_frames:\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Если кадров меньше, чем нужно, дублируем последний кадр вместо нулей\n",
    "    if len(frames) < max_frames and len(frames) > 0:\n",
    "        last_frame = frames[-1]\n",
    "        frames.extend([last_frame for _ in range(max_frames - len(frames))])\n",
    "    elif len(frames) == 0:  # Если кадров нет вообще\n",
    "        frames = [np.zeros((224, 224, 3), dtype=np.uint8) for _ in range(max_frames)]\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519709e0-af8e-4d91-9d79-5674870c567d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2. Извлечение признаков\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Класс для извлечения признаков из видеокадров с использованием предобученной CNN\n",
    "    Основные функции:\n",
    "    - Инициализация предобученной модели CNN (по умолчанию ResNet152)\n",
    "    - Преобразование входных кадров к нужному формату\n",
    "    - Извлечение признаков из каждого кадра\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn_model=None):\n",
    "        if cnn_model is None:\n",
    "            # Загрузка предобученной ResNet152\n",
    "            cnn_model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "            # Удаляем последний классификационный слой\n",
    "            self.model = nn.Sequential(*list(cnn_model.children())[:-1])\n",
    "        else:\n",
    "            self.model = cnn_model\n",
    "\n",
    "        # Перенос модели на устройство (GPU/CPU) и перевод в режим оценки\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Определение преобразований для входных изображений\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),               # Конвертация в PIL Image\n",
    "            transforms.Resize((224, 224)),         # Изменение размера под вход сети\n",
    "            transforms.ToTensor(),                 # Конвертация в тензор\n",
    "            transforms.Normalize(                  # Нормализация\n",
    "                mean=[0.485, 0.456, 0.406],        # Средние значения ImageNet\n",
    "                std=[0.229, 0.224, 0.225]          # Стандартные отклонения ImageNet\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def extract_features(self, frames):\n",
    "        \"\"\"\n",
    "        Извлекает признаки из списка кадров\n",
    "        \n",
    "        Аргументы:\n",
    "            frames (list): Список кадров в формате numpy arrays\n",
    "            \n",
    "        Возвращает:\n",
    "            torch.Tensor: Извлеченные признаки размерности [число_кадров, размерность_признака]\n",
    "            \n",
    "        Процесс работы:\n",
    "        1. Применение преобразований к каждому кадру\n",
    "        2. Извлечение признаков с помощью CNN\n",
    "        3. Накопление и объединение признаков\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        with torch.no_grad(): # Отключаем вычисление градиентов для ускорения\n",
    "            for frame in frames:\n",
    "                # Применяем преобразования и добавляем batch-размерность\n",
    "                frame = self.transform(frame).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Извлекаем признаки\n",
    "                feature = self.model(frame)\n",
    "                feature = feature.squeeze() # Удаляем лишние размерности\n",
    "                \n",
    "                features.append(feature.cpu()) # Переносим на CPU для экономии памяти\n",
    "        \n",
    "        return torch.stack(features) # Объединяем все признаки в один тензор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6fb4d-664f-44ce-92bb-78265e21c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Архитектура модели\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Видео-энкодер для обработки признаков кадров с учетом временной информации\n",
    "    Использует двунаправленный LSTM для анализа последовательности кадров\n",
    "    \n",
    "    Основные функции:\n",
    "    - Обработка признаков отдельных кадров\n",
    "    - Учет временных зависимостей между кадрами\n",
    "    - Подготовка скрытых состояний для декодера\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, hidden_dim, num_layers=1, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Инициализация энкодера\n",
    "        \n",
    "        Аргументы:\n",
    "            feature_dim (int): Размерность входных признаков кадра\n",
    "            hidden_dim (int): Размерность скрытого слоя LSTM\n",
    "            num_layers (int): Количество слоев LSTM\n",
    "            dropout (float): Вероятность дропаута\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Двунаправленный LSTM для временного кодирования\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feature_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,          # Первая размерность - batch\n",
    "            bidirectional=True,       # Двунаправленная архитектура\n",
    "            dropout=dropout if num_layers > 1 else 0  # Дропаут только для многослойных LSTM\n",
    "        )\n",
    "        \n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Прямой проход через энкодер\n",
    "        \n",
    "        Аргументы:\n",
    "            features (torch.Tensor): Признаки видеокадров размерности \n",
    "                                    [batch_size, seq_len, feature_dim]\n",
    "            \n",
    "        Возвращает:\n",
    "            tuple: (outputs, hidden)\n",
    "                - outputs: Выходы LSTM [batch_size, seq_len, hidden_dim*2]\n",
    "                - hidden: Кортеж (скрытое состояние, состояние ячейки)\n",
    "        \"\"\"\n",
    "         # Проверяем размерность входных данных\n",
    "        if features.dim() > 3:\n",
    "            batch_size, seq_len = features.size(0), features.size(1)\n",
    "            # Преобразуем к [batch_size, seq_len, feature_dim]\n",
    "            features = features.view(batch_size, seq_len, -1)\n",
    "            \n",
    "        # Убеждаемся, что последнее измерение имеет правильный размер\n",
    "        if features.size(-1) != self.feature_dim:\n",
    "            raise ValueError(f\"Неверная размерность признаков: ожидается {self.feature_dim}, получено {features.size(-1)}\")\n",
    "        \n",
    "        # Пропускаем признаки через LSTM\n",
    "        outputs, hidden = self.lstm(features)\n",
    "        \n",
    "        return outputs, hidden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
